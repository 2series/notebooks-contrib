{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting NYC Taxi Fares with RAPIDS\n",
    "\n",
    "[RAPIDS](https://rapids.ai/) is a suite of GPU accelerated data science libraries with APIs that should be familiar to users of Pandas, Dask, and Scikitlearn.\n",
    "\n",
    "This notebook focuses on showing how to use cuDF with Dask & XGBoost to scale GPU DataFrame ETL-style operations & model training out to multiple GPUs on mutliple nodes as part of Google Cloud Dataproc.\n",
    "\n",
    "Anaconda has graciously made some of the NYC Taxi dataset available in [a public Google Cloud Storage bucket](https://console.cloud.google.com/storage/browser/anaconda-public-data/nyc-taxi/csv/). We'll use our Dataproc Cluster of GPUs to process it and train a model that predicts the fare amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/envs/rapids/lib/python3.6/site-packages/numba/cuda/envvars.py:16: NumbaDeprecationWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numbapro-environment-variables\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg))\n",
      "/conda/envs/rapids/lib/python3.6/site-packages/numba/cuda/envvars.py:16: NumbaDeprecationWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numbapro-environment-variables\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:44128</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>404.27 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:44128' processes=4 cores=4>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba, xgboost, socket\n",
    "import dask, dask_cudf\n",
    "import time\n",
    "from dask.distributed import Client, wait\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "import glob\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# connect to the Dask cluster created at Dataproc startup time\n",
    "# if you're not using Dataproc, see https://github.com/rapidsai/dask-cuda for help\n",
    "# client = Client(socket.gethostname()+':8786')\n",
    "# forces workers to restart. useful to ensure GPU memory is clear\n",
    "client.restart()\n",
    "\n",
    "# limit work-stealing as much as possible\n",
    "dask.config.set({'distributed.scheduler.work-stealing': False})\n",
    "dask.config.get('distributed.scheduler.work-stealing')\n",
    "dask.config.set({'distributed.scheduler.bandwidth': 1})\n",
    "dask.config.get('distributed.scheduler.bandwidth')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the Data\n",
    "\n",
    "Now that we have a cluster of GPU workers, we'll use [dask-cudf](https://github.com/rapidsai/dask-cudf/) to load and parse a bunch of CSV files into a distributed DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup\n",
    "\n",
    "As usual, the data needs to be massaged a bit before we can start adding features that are useful to an ML model.\n",
    "\n",
    "For example, in the 2014 taxi CSV files, there are `pickup_datetime` and `dropoff_datetime` columns. The 2015 CSVs have `tpep_pickup_datetime` and `tpep_dropoff_datetime`, which are the same columns. One year has `rate_code`, and another `RateCodeID`.\n",
    "\n",
    "Also, some CSV files have column names with extraneous spaces in them.\n",
    "\n",
    "Worst of all, starting in the July 2016 CSVs, pickup & dropoff latitude and longitude data were replaced by location IDs, making the second half of the year useless to us.\n",
    "\n",
    "We'll do a little string manipulation, column renaming, and concatenating of DataFrames to sidestep the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "# %load_ext snakeviz\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'pickup_datetime': 'datetime64[ms]',\n",
    " 'dropoff_datetime': 'datetime64[ms]',\n",
    " 'passenger_count': 'int32',\n",
    " 'trip_distance': 'float32',\n",
    " 'pickup_longitude': 'float32',\n",
    " 'pickup_latitude': 'float32',\n",
    " 'rate_code': 'int32',\n",
    " 'dropoff_longitude': 'float32',\n",
    " 'dropoff_latitude': 'float32',\n",
    " 'fare_amount': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function which takes a DataFrame partition\n",
    "from cudf.dataframe import DataFrame\n",
    "def clean(df_part, remap, must_haves):\n",
    "    # some col-names include pre-pended spaces remove & lowercase column names\n",
    "    tmp = {col: col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(tmp)\n",
    "\n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(remap)\n",
    "\n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part.drop_column(col)\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if col in [\"pickup_datetime\", \"dropoff_datetime\"]:\n",
    "            # handle the datetime columns separetly and fill the null values with an older date of choice\n",
    "            df_part[col] = df_part[col].astype(\"datetime64[ms]\")\n",
    "            df_part[col] = df_part[col].fillna(pd.to_datetime(\"1990-01-02 00:00:00\"))\n",
    "        elif df_part[col].dtype == \"object\":\n",
    "            # handling the rest of the objects\n",
    "            df_part[col] = df_part[col].str.fillna(\"-1\")\n",
    "            df_part[col] = df_part[col].astype(\"float32\")\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if \"int\" in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype(\"int32\")\n",
    "                df_part[col] = df_part[col].fillna(-1)\n",
    "            if \"float\" in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype(\"float32\")\n",
    "                df_part[col] = df_part[col].fillna(-1)\n",
    "\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>-73.990372</td>\n",
       "      <td>40.734695</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.981842</td>\n",
       "      <td>40.732407</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>4.90</td>\n",
       "      <td>-73.980782</td>\n",
       "      <td>40.729912</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.944473</td>\n",
       "      <td>40.716679</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>10.54</td>\n",
       "      <td>-73.984550</td>\n",
       "      <td>40.679565</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.950272</td>\n",
       "      <td>40.788925</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4.75</td>\n",
       "      <td>-73.993469</td>\n",
       "      <td>40.718990</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.962242</td>\n",
       "      <td>40.657333</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1.76</td>\n",
       "      <td>-73.960625</td>\n",
       "      <td>40.781330</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.977264</td>\n",
       "      <td>40.758514</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pickup_datetime dropoff_datetime  passenger_count  trip_distance  \\\n",
       "0      2016-01-01       2016-01-01                2           1.10   \n",
       "1      2016-01-01       2016-01-01                5           4.90   \n",
       "2      2016-01-01       2016-01-01                1          10.54   \n",
       "3      2016-01-01       2016-01-01                1           4.75   \n",
       "4      2016-01-01       2016-01-01                3           1.76   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  rate_code  dropoff_longitude  \\\n",
       "0        -73.990372        40.734695          1         -73.981842   \n",
       "1        -73.980782        40.729912          1         -73.944473   \n",
       "2        -73.984550        40.679565          1         -73.950272   \n",
       "3        -73.993469        40.718990          1         -73.962242   \n",
       "4        -73.960625        40.781330          1         -73.977264   \n",
       "\n",
       "   dropoff_latitude  fare_amount  \n",
       "0         40.732407          7.5  \n",
       "1         40.716679         18.0  \n",
       "2         40.788925         33.0  \n",
       "3         40.657333         16.5  \n",
       "4         40.758514          8.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = \"/home/notebooks/extended/dont_rm_data_taxi/taxi/\"\n",
    "months = [str(x).rjust(2, '0') for x in range(1, 3)]\n",
    "valid_files = [base_path+'yellow_tripdata_2016-'+month+'.csv' for month in months]\n",
    "\n",
    "\n",
    "df_2016_list = []\n",
    "# read & clean 2016 data and concat all DFs\n",
    "\n",
    "for file in valid_files:\n",
    "    \n",
    "\n",
    "    df_2016_list.append(dask_cudf.read_csv(file).map_partitions(clean, remap, must_haves))\n",
    "\n",
    "# concatenate multiple DataFrames into one bigger one\n",
    "taxi_df = dask.dataframe.multi.concat(df_2016_list)\n",
    "\n",
    "taxi_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Interesting Features\n",
    "\n",
    "Dask & cuDF provide standard DataFrame operations, but also let you run \"user defined functions\" on the underlying data.\n",
    "\n",
    "cuDF's [apply_rows](https://rapidsai.github.io/projects/cudf/en/0.6.0/api.html#cudf.dataframe.DataFrame.apply_rows) operation is similar to Pandas's [DataFrame.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html), except that for cuDF, custom Python code is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels.\n",
    "\n",
    "We'll use a Haversine Distance calculation to find total trip distance, and extract additional useful variables from the datetime fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "    \n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    \n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    \n",
    "    df = df.drop('pickup_datetime')\n",
    "    df = df.drop('dropoff_datetime')\n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],outcols=dict(h_distance=np.float32),                   kwargs=dict())\n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 788 ms, sys: 24 ms, total: 812 ms\n",
      "Wall time: 798 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi_df = taxi_df.query(\" and \".join(query_frags))\n",
    "taxi_df = taxi_df.map_partitions(add_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>diff</th>\n",
       "      <th>pickup_latitude_r</th>\n",
       "      <th>pickup_longitude_r</th>\n",
       "      <th>dropoff_latitude_r</th>\n",
       "      <th>dropoff_longitude_r</th>\n",
       "      <th>h_distance</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>-73.990372</td>\n",
       "      <td>40.734695</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.981842</td>\n",
       "      <td>40.732407</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.730000</td>\n",
       "      <td>-74.000000</td>\n",
       "      <td>40.730000</td>\n",
       "      <td>-73.989998</td>\n",
       "      <td>0.762426</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4.90</td>\n",
       "      <td>-73.980782</td>\n",
       "      <td>40.729912</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.944473</td>\n",
       "      <td>40.716679</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.719997</td>\n",
       "      <td>-73.989998</td>\n",
       "      <td>40.709999</td>\n",
       "      <td>-73.949997</td>\n",
       "      <td>3.395178</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10.54</td>\n",
       "      <td>-73.984550</td>\n",
       "      <td>40.679565</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.950272</td>\n",
       "      <td>40.788925</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.669998</td>\n",
       "      <td>-73.989998</td>\n",
       "      <td>40.779999</td>\n",
       "      <td>-73.959999</td>\n",
       "      <td>12.498544</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4.75</td>\n",
       "      <td>-73.993469</td>\n",
       "      <td>40.718990</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.962242</td>\n",
       "      <td>40.657333</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.709999</td>\n",
       "      <td>-74.000000</td>\n",
       "      <td>40.649998</td>\n",
       "      <td>-73.970001</td>\n",
       "      <td>7.344131</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1.76</td>\n",
       "      <td>-73.960625</td>\n",
       "      <td>40.781330</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.977264</td>\n",
       "      <td>40.758514</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.779999</td>\n",
       "      <td>-73.970001</td>\n",
       "      <td>40.750000</td>\n",
       "      <td>-73.979996</td>\n",
       "      <td>2.898252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passenger_count  trip_distance  pickup_longitude  pickup_latitude  \\\n",
       "0                2           1.10        -73.990372        40.734695   \n",
       "1                5           4.90        -73.980782        40.729912   \n",
       "2                1          10.54        -73.984550        40.679565   \n",
       "3                1           4.75        -73.993469        40.718990   \n",
       "4                3           1.76        -73.960625        40.781330   \n",
       "\n",
       "   rate_code  dropoff_longitude  dropoff_latitude  fare_amount  hour  year  \\\n",
       "0          1         -73.981842         40.732407          7.5     0  2016   \n",
       "1          1         -73.944473         40.716679         18.0     0  2016   \n",
       "2          1         -73.950272         40.788925         33.0     0  2016   \n",
       "3          1         -73.962242         40.657333         16.5     0  2016   \n",
       "4          1         -73.977264         40.758514          8.0     0  2016   \n",
       "\n",
       "   month  day  diff  pickup_latitude_r  pickup_longitude_r  \\\n",
       "0      1    1     0          40.730000          -74.000000   \n",
       "1      1    1     0          40.719997          -73.989998   \n",
       "2      1    1     0          40.669998          -73.989998   \n",
       "3      1    1     0          40.709999          -74.000000   \n",
       "4      1    1     0          40.779999          -73.970001   \n",
       "\n",
       "   dropoff_latitude_r  dropoff_longitude_r  h_distance  day_of_week  \\\n",
       "0           40.730000           -73.989998    0.762426          5.0   \n",
       "1           40.709999           -73.949997    3.395178          5.0   \n",
       "2           40.779999           -73.959999   12.498544          5.0   \n",
       "3           40.649998           -73.970001    7.344131          5.0   \n",
       "4           40.750000           -73.979996    2.898252          5.0   \n",
       "\n",
       "   is_weekend  \n",
       "0       False  \n",
       "1       False  \n",
       "2       False  \n",
       "3       False  \n",
       "4       False  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a Training Set\n",
    "\n",
    "Let's imagine you're making a trip to New York on the 25th and want to build a model to predict what fare prices will be like the last few days of the month based on the first part of the month. We'll use a query expression to identify the `day` of the month to use to divide the data into train and test sets.\n",
    "\n",
    "The wall-time below represents how long it takes your GPU cluster to load data from the Google Cloud Storage bucket and the ETL portion of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 104 ms, total: 1.36 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = taxi_df.query('day < 25').persist()\n",
    "\n",
    "# create a Y_train ddf with just the target variable\n",
    "Y_train = X_train[['fare_amount']].persist()\n",
    "# drop the target variable from the training ddf\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]\n",
    "\n",
    "# this wont return until all data is in GPU memory\n",
    "done = wait([X_train, Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16814481"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the XGBoost Regression Model\n",
    "\n",
    "The wall time output below indicates how long it took your GPU cluster to train an XGBoost model over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 68 ms, total: 364 ms\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import dask_xgboost as dxgb_gpu\n",
    "\n",
    "params = {\n",
    " 'learning_rate': 0.3,\n",
    "  'max_depth': 8,\n",
    "  'objective': 'reg:squarederror',\n",
    "  'subsample': 0.6,\n",
    "  'gamma': 1,\n",
    "  'silent': True,\n",
    "  'verbose_eval': True,\n",
    "  'tree_method':'gpu_hist',\n",
    "  'n_gpus': 1\n",
    "}\n",
    "\n",
    "trained_model = dxgb_gpu.train(client, params, X_train, Y_train, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Good is Our Model?\n",
    "\n",
    "Now that we have a trained model, we need to test it with the 25% of records we held out.\n",
    "\n",
    "Based on the filtering conditions applied to this dataset, many of the DataFrame partitions will wind up having 0 rows.\n",
    "\n",
    "This is a problem for XGBoost which doesn't know what to do with 0 length arrays. We'll apply a bit of Dask logic to check for and drop partitions without any rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_partitions(df):\n",
    "    lengths = df.map_partitions(len).compute()\n",
    "    nonempty = [length > 0 for length in lengths]\n",
    "    return df.partitions[nonempty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4339096"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = taxi_df.query('day >= 25').persist()\n",
    "X_test = drop_empty_partitions(X_test)\n",
    "\n",
    "# Create Y_test with just the fare amount\n",
    "Y_test = X_test[['fare_amount']]\n",
    "\n",
    "# Drop the fare amount from X_test\n",
    "X_test = X_test[X_test.columns.difference(['fare_amount'])]\n",
    "\n",
    "# display test set size\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on the test set\n",
    "Y_test['prediction'] = dxgb_gpu.predict(client, trained_model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>prediction</th>\n",
       "      <th>squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.835004</td>\n",
       "      <td>0.027224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>41.0</td>\n",
       "      <td>36.146572</td>\n",
       "      <td>23.555761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.556731</td>\n",
       "      <td>0.196487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4.155344</td>\n",
       "      <td>0.118788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.728344</td>\n",
       "      <td>0.073797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fare_amount  prediction  squared_error\n",
       "176          9.0    8.835004       0.027224\n",
       "177         41.0   36.146572      23.555761\n",
       "178          9.0    8.556731       0.196487\n",
       "179          4.5    4.155344       0.118788\n",
       "180          9.0    8.728344       0.073797"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test['squared_error'] = (Y_test['prediction'] - Y_test['fare_amount'])**2\n",
    "\n",
    "# inspect the results to make sure our calculation looks right\n",
    "Y_test.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.112081468833012"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the actual RMSE over the full test set\n",
    "math.sqrt(Y_test.squared_error.mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trained Model for Later Use\n",
    "\n",
    "To make a model maximally useful, you need to be able to save it for later use.\n",
    "\n",
    "We'll use Google Cloud Storage to persist the trained model in a [dill](https://pypi.org/project/dill/) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs, dill\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "# replace with a bucket you own\n",
    "bucket = 'rapidsai-test-1/'\n",
    "\n",
    "with fs.open(bucket+'trained_model.dill', 'wb') as file:  \n",
    "    dill.dump(trained_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload a Saved Model from Disk\n",
    "\n",
    "You can also read the saved model back out of Google Cloud Storage and into a normal XGBoost model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(bucket+'trained_model.dill', 'rb') as file:  \n",
    "    model_from_disk = dill.load(file)\n",
    "\n",
    "# Generate predictions on the test set again, but this time using the reloaded model\n",
    "Y_test['prediction'] = dxgb_gpu.predict(client, model_from_disk, X_test)\n",
    "\n",
    "# Verify that the predictions result in the same RMSE error\n",
    "Y_test['squared_error'] = (Y_test['prediction'] - Y_test['fare_amount'])**2\n",
    "math.sqrt(Y_test.squared_error.mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "We just demonstrated how to use GPU DataFrames to scale ETL style operations out to multiple GPUs on multiple nodes.\n",
    "\n",
    "We also showed how to pass prepared data directly to XGBoost without having the data ever leave GPU memory. As a result, we can run end to end data processing _and_ model training faster, using less hardware than with a CPU based solution.\n",
    "\n",
    "While other workflows will be more complex or operate on larger dataset sizes, our hope is that pre-processing and training on approximately 70GB (360 million rows) in about 4 minutes shows that GPUs can offer speed ups that give Data Scientists less time to drink coffee, and more time to iterate on and tune model performance.\n",
    "\n",
    "What now?\n",
    "\n",
    "[Check out RAPIDS on GitHub](https://github.com/rapidsai) and follow the development, or pitch in by reporting issues, making pull requests or even just requesting the features your workflows need. We look forward to hearing from you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
