{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation\n",
    "\n",
    "First we want to encode our labels to be between [0, n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "y = train_final_gd['target'].to_array()\n",
    "\n",
    "classes = sorted(np.unique(y))\n",
    "\n",
    "# Build classes with labels from [0, n-1]\n",
    "lbl = LabelEncoder()\n",
    "y = lbl.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our class weights and build a multi-weighted cross-entropy loss function to train our XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "# with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "class_weights = {c: 1 for c in classes}\n",
    "class_weights.update({c:2 for c in [64, 15]})\n",
    "\n",
    "func_loss = partial(xgb_multi_weighted_logloss, \n",
    "                        classes=classes, \n",
    "                        class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess our columns to fill `nan` values with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [i for i in test_final_gd.columns if i not in ['object_id','target']]\n",
    "for col in cols:\n",
    "    train_final_gd[col] = train_final_gd[col].fillna(0).astype('float32')\n",
    "\n",
    "for col in cols:\n",
    "    test_final_gd[col] = test_final_gd[col].fillna(0).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a stratified split of our training dataset into 90% training and 10% validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_final_gd[cols].as_matrix()\n",
    "Xt = test_final_gd[cols].as_matrix()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for XGBoost to build our ensemble of trees   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_params = {\n",
    "            'objective': 'multi:softprob', \n",
    "            'tree_method': 'hist', \n",
    "            'nthread': 16, \n",
    "            'num_class':14,\n",
    "            'max_depth': 7, \n",
    "            'silent':1,\n",
    "            'subsample':0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"tree_method\": \"gpu_hist\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build DMatrix objects with our train, validation, and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(data=X_test, label=y_test)\n",
    "dtest = xgb.DMatrix(data=Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.341401\ttrain-merror:0.275096\teval-wloss:2.05758\ttrain-wloss:1.88726\n",
      "Multiple eval metrics have been passed: 'train-wloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-wloss hasn't improved in 10 rounds.\n",
      "[59]\teval-merror:0.275159\ttrain-merror:0.000849\teval-wloss:1.36472\ttrain-wloss:0.089109\n",
      "\u001b[32mvalidation loss 1.3647\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "clf = xgb.train(gpu_params, \n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=60,\n",
    "                evals=watchlist,\n",
    "                feval=func_loss,\n",
    "                early_stopping_rounds=10,\n",
    "                verbose_eval=1000)\n",
    "\n",
    "yp = clf.predict(dvalid)\n",
    "\n",
    "gpu_loss = multi_weighted_logloss(y_test, yp, classes, class_weights)\n",
    "\n",
    "ysub = clf.predict(dtest)\n",
    "\n",
    "line = 'validation loss %.4f'%gpu_loss\n",
    "print(colored(line,'green'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuml4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
