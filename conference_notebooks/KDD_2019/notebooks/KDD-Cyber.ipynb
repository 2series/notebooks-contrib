{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYBER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer networks generate massive amounts of heterogeneous data as users interact with other users, computers, and services. These interactions can be modeled as large, heterogeneous property graphs, with multidimensional characteristics of the communication embedded on an edge connecting nodes. Current techniques to identify subgraph evolution over time and extract anomalies require lengthy compute times or necessitate significant pre-filtering of the graph. In this tutorial, we showcase an approach to flagging anomalous network communications in a large graph using a combination of structural graph features and graph analytics, running end-to-end in RAPIDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import cugraph\n",
    "import cudf\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import nvstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load strong_cc.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Stringly Connected Components\n",
    "\n",
    "# Import needed libraries\n",
    "import cugraph\n",
    "import cudf\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ## Prep\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# prep\n",
    "# - create a second dataframe that contains a list of vertices and their connected component ID\n",
    "# - the column name is set to 'src' to allow for joining, but it is really a list of all vertices \n",
    "# - Input:  \n",
    "#      _coo  = the COO dataframe\n",
    "#      _s    = symetermized => 1 = yes, 0 = no.  If no then 'src' and 'dst' need to be combined\n",
    "def prep(_coo, s) :\n",
    "    _vert = cudf.DataFrame()\n",
    "\n",
    "    if s :\n",
    "        _vert['src'] = _coo['src'].unique()\n",
    "   \n",
    "    else :\n",
    "        # combine the source and destination \n",
    "        tmp = cudf.DataFrame()\n",
    "        tmp['id'] = _coo['src'].append(_coo['dst'])\n",
    "        _vert['src'] = tmp['id'].unique()\n",
    "        \n",
    "        del tmp\n",
    "\n",
    "    # starting ID are vertex IDs\n",
    "    _vert['cc'] = _vert['src']\n",
    "    \n",
    "\n",
    "    return _vert\n",
    "\n",
    "\n",
    "# ## Loop\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# define a kernel to update the row\n",
    "# this will add an extra column indicating whether or not the value chnaged\n",
    "def update_row(src, cc, cc2, out1, s2, kwargs) :\n",
    "    for i, (a, b, c) in enumerate(zip(src, cc, cc2)) :\n",
    "        if ( c == -1) :\n",
    "            s2[i] = 0\n",
    "        elif ( c < b ) :\n",
    "            out1[i] = c\n",
    "            s2[i] = 1\n",
    "        else :\n",
    "            out1[i] = b\n",
    "            s2[i] = 0\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def propogate(_coo, _vert) :\n",
    "    # merge (join) on src\n",
    "    tmp = _coo.merge(_vert, on=['src'], how='left')    \n",
    "    \n",
    "    # drop 'src' since we just push src values to 'dst'\n",
    "    tmp = tmp.drop(['src'])\n",
    "    \n",
    "    aggs = OrderedDict()\n",
    "    aggs['cc'] = 'min'\n",
    "    \n",
    "    t2 = tmp.groupby(['dst'], as_index=False).agg(aggs)    \n",
    "\n",
    "    return t2\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def update_cc(_vert, _tmp) :\n",
    "    # need to rename the t2 columns\n",
    "    t2 = _tmp.rename(columns={'dst':'src','cc': 'cc2'})\n",
    "    \n",
    "    #now merge _tmp in _vert\n",
    "    _v = _vert.merge(t2, on=['src'], how='left') \n",
    "    \n",
    "    z = _v.apply_rows(update_row, \n",
    "                        incols=['src', 'cc', 'cc2'],\n",
    "                        outcols=dict(out1=np.int64, s2=np.bool),\n",
    "                        kwargs=dict(kwargs=1)\n",
    "                       )\n",
    "    \n",
    "    status = z['s2'].max()\n",
    "    \n",
    "    z.drop_column('cc')\n",
    "    z.drop_column('cc2')\n",
    "    z.drop_column('s2')\n",
    "    z = z.rename(columns={'out1':'cc'})   \n",
    "    \n",
    "    return status, z\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Run the cuGraph Louvain analytic (using nvGRAPH function)\n",
    "def strong_cc(coo_df):\n",
    "    \n",
    "    vert_gdf = prep(coo_df, False)\n",
    "    status = 1\n",
    "    loop = 0\n",
    "    \n",
    "    while status == 1 :\n",
    "        loop = loop + 1\n",
    "\n",
    "        tmp1 = propogate(coo_df, vert_gdf)\n",
    "        status, vert_gdf = update_cc(vert_gdf, tmp1)  \n",
    "    # end while\n",
    "\n",
    "    \n",
    "    return vert_gdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Step - load and prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Download the four UNSW-NB15 data files\n",
    "#!wget -N https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/UNSW-NB15_1.csv\n",
    "#!wget -N https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/UNSW-NB15_2.csv\n",
    "#!wget -N https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/UNSW-NB15_3.csv\n",
    "#!wget -N https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/UNSW-NB15_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles = [\n",
    "    'UNSW-NB15_1.csv',\n",
    "    'UNSW-NB15_2.csv',\n",
    "    'UNSW-NB15_3.csv',\n",
    "    'UNSW-NB15_4.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'srcip',\n",
    "    'sport',\n",
    "    'dstip',\n",
    "    'dsport',\n",
    "    'proto',\n",
    "    'state',\n",
    "    'dur',\n",
    "    'sbytes',\n",
    "    'dbytes',\n",
    "    'sttl',\n",
    "    'dttl',\n",
    "    'sloss',\n",
    "    'dloss',\n",
    "    'service',\n",
    "    'Sload',\n",
    "    'Dload',\n",
    "    'Spkts',\n",
    "    'Dpkts',\n",
    "    'swin',\n",
    "    'dwin',\n",
    "    'stcpb',\n",
    "    'dtcpb',\n",
    "    'smeansz',\n",
    "    'dmeansz',\n",
    "    'trans_depth',\n",
    "    'res_bdy_len',\n",
    "    'Sjit',\n",
    "    'Djit',\n",
    "    'Stime',\n",
    "    'Ltime',\n",
    "    'Sintpkt',\n",
    "    'Dintpkt',\n",
    "    'tcprtt',\n",
    "    'synack',\n",
    "    'ackdat',\n",
    "    'is_sm_ips_ports',\n",
    "    'ct_state_ttl',\n",
    "    'ct_flw_http_mthd',\n",
    "    'is_ftp_login',\n",
    "    'ct_ftp_cmd',\n",
    "    'ct_srv_src',\n",
    "    'ct_srv_dst',\n",
    "    'ct_dst_ltm',\n",
    "    'ct_src_ ltm',\n",
    "    'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_ltm',\n",
    "    'ct_dst_src_ltm',\n",
    "    'attack_cat',\n",
    "    'Label'\n",
    "]\n",
    "\n",
    "col_dtypes = OrderedDict([\n",
    "    ('srcip', 'str'),\n",
    "    ('sport', 'int32'),\n",
    "    ('dstip', 'str'),\n",
    "    ('dsport', 'int64'),\n",
    "    ('proto', 'str'),\n",
    "    ('state', 'str'),\n",
    "    ('dur', 'float64'),\n",
    "    ('sbytes', 'int64'),\n",
    "    ('dbytes', 'int64'),\n",
    "    ('sttl', 'int64'),\n",
    "    ('dttl', 'int64'),\n",
    "    ('sloss', 'int64'),\n",
    "    ('dloss', 'int64'),\n",
    "    ('service', 'str'),\n",
    "    ('Sload', 'float64'),\n",
    "    ('Dload', 'float64'),\n",
    "    ('Spkts', 'int64'),\n",
    "    ('Dpkts', 'int64'),\n",
    "    ('swin', 'int64'),\n",
    "    ('dwin', 'int64'),\n",
    "    ('stcpb', 'int64'),\n",
    "    ('dtcpb', 'int64'),\n",
    "    ('smeansz', 'int64'),\n",
    "    ('dmeansz', 'int64'),\n",
    "    ('trans_depth', 'int64'),\n",
    "    ('res_bdy_len', 'int64'),\n",
    "    ('Sjit', 'float64'),\n",
    "    ('Djit', 'float64'),\n",
    "    ('Stime', 'str'),\n",
    "    ('Ltime', 'str'),\n",
    "    ('Sintpkt', 'float64'),\n",
    "    ('Dintpkt', 'float64'),\n",
    "    ('tcprtt', 'float64'),\n",
    "    ('synack', 'float64'),\n",
    "    ('ackdat', 'float64'),\n",
    "    ('is_sm_ips_ports', 'int8'),\n",
    "    ('ct_state_ttl', 'int64'),\n",
    "    ('ct_flw_http_mthd', 'int64'),\n",
    "    ('is_ftp_login', 'int8'),\n",
    "    ('ct_ftp_cmd', 'int64'),\n",
    "    ('ct_srv_src', 'int64'),\n",
    "    ('ct_srv_dst', 'int64'),\n",
    "    ('ct_dst_ltm', 'int64'),\n",
    "    ('ct_src_ ltm', 'int64'),\n",
    "    ('ct_src_dport_ltm', 'int64'),\n",
    "    ('ct_dst_sport_ltm', 'int64'),\n",
    "    ('ct_dst_src_ltm', 'int64'),\n",
    "    ('attack_cat', 'str'),\n",
    "    ('Label', 'int8')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read COO data.  Input are f = file name, sl = skip lines, d = delimiter\n",
    "def read_data(f, c, dt) :\n",
    "    print(\"reading \" + f)\n",
    "    return cudf.read_csv(f, names=c, delimiter=',', dtype=list(dt.values()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will add two new columns to the dataframe\n",
    "def ip_str_to_int(_gdf) :\n",
    "    # convert the String IP addresses into integer values\n",
    "    _gdf['src'] = _gdf['srcip'].str.ip2int()\n",
    "    _gdf['dst'] = _gdf['dstip'].str.ip2int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1 = read_data(datafiles[0], cols, col_dtypes )\n",
    "gdf2 = read_data(datafiles[1], cols, col_dtypes )\n",
    "gdf3 = read_data(datafiles[2], cols, col_dtypes )\n",
    "gdf4 = read_data(datafiles[3], cols, col_dtypes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data sets together\n",
    "gdf = cudf.concat([gdf1, gdf2, gdf3, gdf4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup to reclaim space\n",
    "del gdf1\n",
    "del gdf2\n",
    "del gdf3\n",
    "del gdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the String IP addresses into integer values\n",
    "ip_str_to_int( gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['attack_cat'].null_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head(2).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo = cudf.DataFrame()\n",
    "coo['src'] = gdf['src']\n",
    "coo['dst'] = gdf['dst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = strong_cc(coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v['cc'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = coo.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdf)) :\n",
    "    G.add_edge(pdf['src'].iloc[i], pdf['dst'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['src'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
