{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rossmann Store Sales Prediction Example\n",
    "In this example, we'll illustrate how to use NVTabular to preprocess and load tabular data for training neural networks into TensorFlow. This usees a [dataset built by FastAI](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb) for solving the [Kaggle Rossmann Store Sales competition](https://www.kaggle.com/c/rossmann-store-sales). To expedite this tutorial, we've already lightly preprocessed the data we'll be using. For a full version of this specific example, please visit the [NVTabular GitHub](https://github.com/NVIDIA/NVTabular)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into full details of the Rossmann Kaggle competition, but below is a brief description of the task taken directly from Kaggle:\n",
    "\n",
    "<blockquote>Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, our goal is to use features in this dataset to predict sales. There are a number of different ways to do this. You could employ a random forest classifier or even a Naive Bayes model. And in practice, it's good to try a variety of models and cross validate. However, for the purposes of this tutorial, we want to illustrate how easy it is to use data loaders and features built into NVTabular to create a deep learning model with tabular data.\n",
    "\n",
    "Here we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we have a GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to check. Really hope we do, otherwise this is going to be a lightening fast tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 15 19:02:56 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:15:00.0 Off |                  Off |\n",
      "| 33%   31C    P8    12W / 260W |      1MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     Off  | 00000000:2D:00.0 Off |                  Off |\n",
      "| 33%   33C    P8    23W / 260W |    140MiB / 48592MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we have the necessary imports. We'll primarily be using NVTabular, cuDF, and TensorFlow (which we'll import a bit later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular as nvt\n",
    "import os\n",
    "import glob\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our Dataset\n",
    "Let's start by defining some of the _a priori_ information about our data, including its schema (what columns to use and what sorts of variables they represent), as well as the location of the files corresponding to some particular sampling from this schema. Note that throughout, I'll use UPPERCASE variables to represent this sort of a priori information that you might usually encode using commandline arguments or config files.\n",
    "\n",
    "For ease of this tutorial, we've already lightly preprocessed the input data and generated nice, clean CSV files for you. You know, just like in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get('DATA_DIR', '/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What files are available to train on in our data directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jp_ross  test.csv  train.csv  valid.csv\n"
     ]
    }
   ],
   "source": [
    "! ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train.csv` and `valid.csv` seem like good candidates, let's use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "VALID_PATH = os.path.join(DATA_DIR, 'valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we set about modeling, we can explore the data using cuDF. Let's just read in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cudf.read_csv(TRAIN_PATH, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Year</th>\n",
       "      <th>...</th>\n",
       "      <th>AfterStateHoliday</th>\n",
       "      <th>BeforeStateHoliday</th>\n",
       "      <th>AfterPromo</th>\n",
       "      <th>BeforePromo</th>\n",
       "      <th>SchoolHoliday_bw</th>\n",
       "      <th>StateHoliday_bw</th>\n",
       "      <th>Promo_bw</th>\n",
       "      <th>SchoolHoliday_fw</th>\n",
       "      <th>StateHoliday_fw</th>\n",
       "      <th>Promo_fw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1097</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>5961</td>\n",
       "      <td>1405</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>4220</td>\n",
       "      <td>619</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>6851</td>\n",
       "      <td>1444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>17267</td>\n",
       "      <td>2875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>3102</td>\n",
       "      <td>729</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek        Date  Sales  Customers  Open  Promo  StateHoliday  \\\n",
       "0   1097          2  2013-01-01   5961       1405     1      0          True   \n",
       "1     85          2  2013-01-01   4220        619     1      0          True   \n",
       "2    259          2  2013-01-01   6851       1444     1      0          True   \n",
       "3    262          2  2013-01-01  17267       2875     1      0          True   \n",
       "4    274          2  2013-01-01   3102        729     1      0          True   \n",
       "\n",
       "   SchoolHoliday  Year  ...  AfterStateHoliday  BeforeStateHoliday  \\\n",
       "0              1  2013  ...                0.0                 0.0   \n",
       "1              1  2013  ...                0.0                 0.0   \n",
       "2              1  2013  ...                0.0                 0.0   \n",
       "3              1  2013  ...                0.0                 0.0   \n",
       "4              1  2013  ...                0.0                 0.0   \n",
       "\n",
       "   AfterPromo  BeforePromo  SchoolHoliday_bw  StateHoliday_bw  Promo_bw  \\\n",
       "0         0.0          0.0               1.0              1.0       0.0   \n",
       "1         0.0          0.0               1.0              1.0       0.0   \n",
       "2         0.0          0.0               1.0              1.0       0.0   \n",
       "3         0.0          0.0               1.0              1.0       0.0   \n",
       "4         0.0          0.0               1.0              1.0       0.0   \n",
       "\n",
       "   SchoolHoliday_fw  StateHoliday_fw  Promo_fw  \n",
       "0               4.0              1.0       1.0  \n",
       "1               4.0              1.0       1.0  \n",
       "2               7.0              1.0       1.0  \n",
       "3               4.0              1.0       1.0  \n",
       "4               7.0              1.0       1.0  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is fairly wide, so we can select just one record and look at what the typical data is. cuDF doesn't support non-numeric types in the `values` call yet, but it's easy to take a small amount of data to Pandas to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1097, 2, '2013-01-01', 5961, 1405, 1, 0, True, 1, 2013, 1, 1, 1,\n",
       "        'b', 'b', 720.0, 3, 2002, 0, 1, 1900, None, 'RP',\n",
       "        'Rossmann_DE_RP', '2013-01-06 - 2013-01-12', 36, 'Rossmann_DE',\n",
       "        '2013-01-06 - 2013-01-12', 62, '2013-01-06', -1, 1, 6, 8, 6, 4,\n",
       "        7, 3, 1, 92, 75, 59, 1015, 1010, 1008, 31.0, 19.0, 5.0, 26, 19,\n",
       "        nan, 0.25, 8.0, 'Rain', -1, 'RheinlandPfalz', '2002-03-15', 3945,\n",
       "        24, '1900-01-08', 0, 0, -1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n",
       "        1.0, 0.0, 4.0, 1.0, 1.0]], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[0:0].to_pandas().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the data types of the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('bool'): Index(['StateHoliday'], dtype='object'),\n",
       " dtype('int8'): Index(['State_DE', 'Id'], dtype='object'),\n",
       " dtype('int64'): Index(['Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo',\n",
       "        'SchoolHoliday', 'Year', 'Month', 'Week', 'Day',\n",
       "        'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
       "        'Promo2SinceWeek', 'Promo2SinceYear', 'trend', 'trend_DE', 'Month_DE',\n",
       "        'Day_DE', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
       "        'Dew_PointC', 'MeanDew_PointC', 'Min_DewpointC', 'Max_Humidity',\n",
       "        'Mean_Humidity', 'Min_Humidity', 'Max_Sea_Level_PressurehPa',\n",
       "        'Mean_Sea_Level_PressurehPa', 'Min_Sea_Level_PressurehPa',\n",
       "        'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', 'WindDirDegrees',\n",
       "        'CompetitionDaysOpen', 'CompetitionMonthsOpen', 'Promo2Days',\n",
       "        'Promo2Weeks'],\n",
       "       dtype='object'),\n",
       " dtype('float64'): Index(['CompetitionDistance', 'Max_VisibilityKm', 'Mean_VisibilityKm',\n",
       "        'Min_VisibilitykM', 'Max_Gust_SpeedKm_h', 'Precipitationmm',\n",
       "        'CloudCover', 'AfterSchoolHoliday', 'BeforeSchoolHoliday',\n",
       "        'AfterStateHoliday', 'BeforeStateHoliday', 'AfterPromo', 'BeforePromo',\n",
       "        'SchoolHoliday_bw', 'StateHoliday_bw', 'Promo_bw', 'SchoolHoliday_fw',\n",
       "        'StateHoliday_fw', 'Promo_fw'],\n",
       "       dtype='object'),\n",
       " dtype('O'): Index(['Date', 'StoreType', 'Assortment', 'PromoInterval', 'State', 'file',\n",
       "        'week', 'file_DE', 'week_DE', 'Date_DE', 'Events', 'StateName',\n",
       "        'CompetitionOpenSince', 'Promo2Since'],\n",
       "       dtype='object')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns.to_series().groupby(train_df.dtypes).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By repeating this process, we can assign columns into variables that link common data types. We're looking to predict `Sales`, so we'll denote that as our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "LABEL_COLUMNS = ['Sales']\n",
    "\n",
    "COLUMNS = CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflows and Preprocessing\n",
    "A `Workflow` is used to represent the chains of feature engineering and preprocessing operations performed on a dataset, and is instantiated with a description of the dataset's schema so that it can keep track of how columns transform with each operation.\n",
    "\n",
    "_NOTE: As of this tutorial, NVT doesn't support transforming label columns. We'll pretend it's a regular continuous column during our feature engineering phase._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS+LABEL_COLUMNS,\n",
    "    label_name=LABEL_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Operations to our Workflow\n",
    "We add operations to a `Workflow` by leveraging the `add_(cat|cont)_feature` and `add_(cat|cont)_preprocess` methods for categorical and continuous variables, respectively. When we're done adding ops, we call the `finalize` method to let the `Workflow` build  a representation of its outputs. We use these operations to fill missing values, standardize the `Sales` column around 0 with a standard deviation of 1 (`LogOp`), normalize continuous columns, and transform categorical features into unique integer values (`Categorify`). Complete details about these functions are available on the [NVTabular's API documention site](https://nvidia.github.io/NVTabular/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.add_cont_feature(nvt.ops.FillMissing())\n",
    "proc.add_cont_preprocess(nvt.ops.LogOp(columns=['Sales']))\n",
    "proc.add_cont_preprocess(nvt.ops.Normalize())\n",
    "proc.add_cat_preprocess(nvt.ops.Categorify())\n",
    "proc.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "In general, the `Ops` in our `Workflow` will require measurements of statistical properties of our data in order to be leveraged. For example, the `Normalize` op requires measurements of the dataset mean and standard deviation, and the `Categorify` op requires an accounting of all the categories a particular feature can manifest. However, we frequently need to measure these properties across datasets which are too large to fit into GPU memory (or CPU memory for that matter) at once.\n",
    "\n",
    "NVTabular solves this by providing the `dataset` object, an iterator over manageable chunks of sets of parquet or csv files that can we can use to compute statistics in an online fashion (and, later, to train neural networks in batches loaded from disk). The size of those chunks will be determined by the `gpu_memory_frac` kwarg, which will load chunks whose memory footprint is equal to that fraction of available GPU memory.\n",
    "\n",
    "Larger chunks will lead to shorter run times due to the parallel-processing power of GPUs, but will constrain your memory and possibly lead to disk caching by expensive operations, thereby lowering efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_MEMORY_FRAC = 0.2\n",
    "train_ds_iterator = nvt.dataset(TRAIN_PATH, gpu_memory_frac=GPU_MEMORY_FRAC, columns=COLUMNS)\n",
    "valid_ds_iterator = nvt.dataset(VALID_PATH, gpu_memory_frac=GPU_MEMORY_FRAC, columns=COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our datasets, we'll apply our `Workflow` to them and save the results out to parquet files for fast reading at train time. We'll also measure and record statistics on our training set using the `record_stats=True` kwarg so that our `Workflow` can use them at apply time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESS_DIR = os.path.join(DATA_DIR, 'jp_ross')\n",
    "PREPROCESS_DIR_TRAIN = os.path.join(PREPROCESS_DIR, 'train')\n",
    "PREPROCESS_DIR_VALID = os.path.join(PREPROCESS_DIR, 'valid')\n",
    "! mkdir -p $PREPROCESS_DIR_TRAIN\n",
    "! mkdir -p $PREPROCESS_DIR_VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.apply(train_ds_iterator, apply_offline=True, record_stats=True, output_path=PREPROCESS_DIR_TRAIN, shuffle=False)\n",
    "proc.apply(valid_ds_iterator, apply_offline=True, record_stats=False, output_path=PREPROCESS_DIR_VALID, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Columns\n",
    "The workflow will leverage the `Workflow.ds_to_tensors` method, which will map a dataset to its corresponding tensors. In order to make sure it runs correctly, we'll call the `create_final_cols` method to let the `Workflow` know to build the output dataset schema, and then we'll be sure to remove instances of the label column that got added to that schema when we performed processing on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.create_final_cols()\n",
    "# using log op and normalize on sales column causes it to get added to\n",
    "# continuous columns_ctx, so we'll remove it here\n",
    "while True:\n",
    "    try:\n",
    "        proc.columns_ctx['final']['cols']['continuous'].remove(LABEL_COLUMNS[0])\n",
    "    except ValueError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network\n",
    "Now that our data is preprocessed and saved out, we can leverage `dataset`s to read through the preprocessed parquet files in an online fashion to train neural networks! Even better, using the `dlpack` library, we can pass data loaded by cuDF's accelerated parquet reader to networks in TensorFlow.\n",
    "\n",
    "We'll start by setting some universal hyperparameters for our model and optimizer (without making any claims on the quality of these hyperparmeter choices). We leave it as an exercise to the attendee to experiment with the hyperparemeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 65536\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_DROPOUT_RATE = 0.04\n",
    "DROPOUT_RATES = [0.001, 0.01]\n",
    "HIDDEN_DIMS = [1000, 500]\n",
    "EPOCHS = 10\n",
    "\n",
    "# our categorical encoder provides a handy utility for coming up with default embedding sizes\n",
    "# based on the number of potential categories, so we'll just use those defaults\n",
    "EMBEDDING_TABLE_SHAPES = {\n",
    "    column: shape for column, shape in\n",
    "        proc.df_ops['Categorify'].get_emb_sz(\n",
    "            proc.stats['categories'], proc.columns_ctx['categorical']['base']\n",
    "        )\n",
    "}\n",
    "\n",
    "TRAIN_PATHS = sorted(glob.glob(os.path.join(PREPROCESS_DIR_TRAIN, '*.parquet')))\n",
    "VALID_PATHS = sorted(glob.glob(os.path.join(PREPROCESS_DIR_VALID, '*.parquet')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "The first thing we need to do is set up the objects for getting data into our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KerasSequenceDataset` wraps a lightweight iterator around a `dataset` object to handle chunking, shuffling, and application of any workflows (which can be applied online as a preprocessing step). For column names, can use either a list of string names or a list of TensorFlow `feature_columns` that will be used to feed the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/nvtabular-0.1.1-py3.7.egg/nvtabular/tf_dataloader.py:37: UserWarning: Tensorflow 2.2.0 dlpack integration has known memory leak issues\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# it's too late and TF will have claimed all free GPU memory\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"8192\" # explicit MB\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"0.5\" # fraction of free memory\n",
    "from nvtabular.tf_dataloader import KerasSequenceDataset\n",
    "\n",
    "# cheap wrapper to keep things some semblance of neat\n",
    "def make_categorical_embedding_column(name, dictionary_size, embedding_dim):\n",
    "    return tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_identity(name, dictionary_size),\n",
    "        embedding_dim\n",
    "    )\n",
    "\n",
    "# instantiate our columns\n",
    "categorical_columns = [\n",
    "    make_categorical_embedding_column(name, *EMBEDDING_TABLE_SHAPES[name]) for\n",
    "        name in CATEGORICAL_COLUMNS\n",
    "]\n",
    "continuous_columns = [\n",
    "    tf.feature_column.numeric_column(name, (1,)) for name in CONTINUOUS_COLUMNS\n",
    "]\n",
    "\n",
    "# feed them to our datasets\n",
    "train_dataset_tf = KerasSequenceDataset(\n",
    "    TRAIN_PATHS, # you could also use a glob pattern\n",
    "    categorical_columns+continuous_columns,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_name=LABEL_COLUMNS[0],\n",
    "    shuffle=True,\n",
    "    buffer_size=48 # how many batches to load at once\n",
    ")\n",
    "valid_dataset_tf = KerasSequenceDataset(\n",
    "    VALID_PATHS, # you could also use a glob pattern\n",
    "    categorical_columns+continuous_columns,\n",
    "    batch_size=BATCH_SIZE*4,\n",
    "    label_name=LABEL_COLUMNS[0],\n",
    "    shuffle=False,\n",
    "    buffer_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Model\n",
    "Next we'll need to define the inputs that will feed our model and build an architecture on top of them. For now, we'll just stick to a simple MLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras, we can define the layers of our model and their parameters explicitly. Here, for the sake of consistency, I've tried to recreate the model created by FastAI as faithfully as I can given their description [here](https://docs.fast.ai/tabular.models.html#TabularModel), without making any claims as to whether this is the _right_ model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseFeatures layer needs a dictionary of {feature_name: input}\n",
    "categorical_inputs = {}\n",
    "for column_name in CATEGORICAL_COLUMNS:\n",
    "    categorical_inputs[column_name] = tf.keras.Input(name=column_name, shape=(1,), dtype=tf.int64)\n",
    "categorical_embedding_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
    "categorical_x = categorical_embedding_layer(categorical_inputs)\n",
    "categorical_x = tf.keras.layers.Dropout(EMBEDDING_DROPOUT_RATE)(categorical_x)\n",
    "\n",
    "# Just concatenating continuous, so can use a list\n",
    "continuous_inputs = []\n",
    "for column_name in CONTINUOUS_COLUMNS:\n",
    "    continuous_inputs.append(tf.keras.Input(name=column_name, shape=(1,), dtype=tf.float32))\n",
    "continuous_embedding_layer = tf.keras.layers.Concatenate(axis=1)\n",
    "continuous_x = continuous_embedding_layer(continuous_inputs)\n",
    "continuous_x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)(continuous_x)\n",
    "\n",
    "# concatenate and build MLP\n",
    "x = tf.keras.layers.Concatenate(axis=1)([categorical_x, continuous_x])\n",
    "for dim, dropout_rate in zip(HIDDEN_DIMS, DROPOUT_RATES):\n",
    "    x = tf.keras.layers.Dense(dim, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "# combine all our inputs into a single list\n",
    "# (note that you can still use .fit, .predict, etc. on a dict\n",
    "# that maps input tensor names to input values)\n",
    "inputs = list(categorical_inputs.values()) + continuous_inputs\n",
    "tf_model = tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer and Train\n",
    "This is probably the most conceptually consistent part between the frameworks: we'll define an objective and a method for optimizing it, then fit our model to our dataset iterators using that optimization scheme. We'll build a quick implementation of the metric Kaggle used in the original competition so that we can keep tabs on it during training.\n",
    "\n",
    "Submissions to the Rossmann Store Sales Kaggle competition were evaulated using Root Mean Square Percentage Error (RMSPE).\n",
    "\n",
    "$$\\textrm{RMSPE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left ( \\frac{y_i-\\hat{y_i}}{y_i} \\right )^2}$$\n",
    "\n",
    "Note that we're making an explicit choice to drop zeroes to maintain consistency with Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 3s 302ms/step - loss: 3.5434 - rmspe_tf: 1.4428 - val_loss: 1.4955 - val_rmspe_tf: 0.6983\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 2s 169ms/step - loss: 1.2781 - rmspe_tf: 0.6337 - val_loss: 1.0480 - val_rmspe_tf: 0.5513\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 2s 168ms/step - loss: 0.8392 - rmspe_tf: 0.4786 - val_loss: 0.8354 - val_rmspe_tf: 0.4091\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 2s 167ms/step - loss: 0.6714 - rmspe_tf: 0.4329 - val_loss: 0.6871 - val_rmspe_tf: 0.4360\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 2s 172ms/step - loss: 0.5752 - rmspe_tf: 0.4023 - val_loss: 0.5974 - val_rmspe_tf: 0.3811\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 2s 175ms/step - loss: 0.5051 - rmspe_tf: 0.3705 - val_loss: 0.5514 - val_rmspe_tf: 0.3684\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 2s 173ms/step - loss: 0.4427 - rmspe_tf: 0.3590 - val_loss: 0.4884 - val_rmspe_tf: 0.3493\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 2s 175ms/step - loss: 0.3842 - rmspe_tf: 0.3208 - val_loss: 0.4335 - val_rmspe_tf: 0.3290\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 2s 177ms/step - loss: 0.3346 - rmspe_tf: 0.3099 - val_loss: 0.3913 - val_rmspe_tf: 0.3132\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 2s 177ms/step - loss: 0.2938 - rmspe_tf: 0.2966 - val_loss: 0.3546 - val_rmspe_tf: 0.2954\n"
     ]
    }
   ],
   "source": [
    "def rmspe_tf(y_true, y_pred):\n",
    "    # map back into \"true\" space by undoing transform\n",
    "    y_true = y_true*proc.stats['stds']['Sales'] + proc.stats['means']['Sales']\n",
    "    y_pred = y_pred*proc.stats['stds']['Sales'] + proc.stats['means']['Sales']\n",
    "\n",
    "    # and then the log(1+x)\n",
    "    y_true = tf.exp(y_true) - 1\n",
    "    y_pred = tf.exp(y_pred) - 1\n",
    "\n",
    "    # drop zeroes for stability (and consistency with Kaggle)\n",
    "    where = tf.not_equal(y_true, 0.)\n",
    "    y_true = y_true[where]\n",
    "    y_pred = y_pred[where]\n",
    "\n",
    "    percent_error = (y_true - y_pred) / y_true\n",
    "    return tf.sqrt(tf.reduce_mean(percent_error**2))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "tf_model.compile(optimizer, 'mse', metrics=[rmspe_tf])\n",
    "history = tf_model.fit(\n",
    "    train_dataset_tf,\n",
    "    validation_data=valid_dataset_tf,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does fairly well straight away, with minimal tuning of hyperparemeters and thought given to specific features and feature engineering. We also could reconsider the network structure, opting for something other than a simple MPE. In reality, it would take a RMSPE <= 0.10021 to beat the eventual winner of this specific Kaggle compeition. So while this submission won't win that Kaggle competition, the hope was to illustrate how easy it is to process data using NVTabular and feed it to a neural network in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
