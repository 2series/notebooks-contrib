{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a cuStreamz job for the classic example of Streaming Word Count.**\n",
    "\n",
    "For this example, we will be demonstrating how to stream from a textfile. Please install pytest using conda to use the tmpfile() function from streamz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streamz and cudf imports\n",
    "import cudf\n",
    "from streamz import Stream\n",
    "from streamz.dataframe import DataFrame\n",
    "from streamz.utils_test import tmpfile\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the data coming in to a textfile and each line is in the form of \"this is line x\", where x is an incremental counter.\n",
    "\n",
    "Now, we write a function to parse each line to get the list of words in each line.\n",
    "\n",
    "One can also make use of nvstrings (now custrings, the GPU-accelerated string manipulation library) to tokenise each line. Refer to process_line_nvstrings()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    words = line.strip('\\n').split(\" \")\n",
    "    return words\n",
    "\n",
    "import nvstrings, nvtext\n",
    "def process_line_nvstrings(line):\n",
    "    device_line = nvstrings.to_device(line)\n",
    "    words = nvtext.tokenize(device_line)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a temporary textfile using tmpfile() which streamz.utils_test provides to simulate streaming word count from a textfile.\n",
    "\n",
    "*One can write a separate function to write continuously to a textfile, and still use the same cuStreamz code as shown below to calculate word count.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count\n",
      "9      1\n",
      "is     10\n",
      "line     10\n",
      "this     10\n",
      "   count\n",
      "9      1\n",
      "is     20\n",
      "line     20\n",
      "this     20\n"
     ]
    }
   ],
   "source": [
    "with tmpfile() as fn:\n",
    "    with open(fn, 'wt') as f:\n",
    "        #Write some random data to the file\n",
    "        for i in range(0,10):\n",
    "            f.write(\"this is line \" + str(i) + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        #Create a stream from the textfile, and specify the interval to poll the file at.\n",
    "        source = Stream.from_textfile(fn, poll_interval=0.01, \\\n",
    "                                 asynchronous=True, start=False)\n",
    "        \n",
    "        #Apply the process_line helper function on each element/line streamed from the textfile.\n",
    "        stream = source.map(process_line)\n",
    "        \n",
    "        '''\n",
    "        Streamz DataFrame does the trick!\n",
    "        \n",
    "        After we get the parsed word list on our stream from the textfile, \n",
    "        we just perform simple aggregations using the Streamz DataFrame to get the word count.\n",
    "        \n",
    "        We then write the output (word count) to a list.\n",
    "        '''\n",
    "        stream_df = stream.map(lambda words: cudf.DataFrame({'word': words, 'count': np.ones(len(words),dtype=np.int32)}))\n",
    "        sdf = DataFrame(stream_df, example=cudf.DataFrame({'word':[], 'count':[]}))\n",
    "        output = sdf.groupby('word').sum().stream.gather().sink_to_list()\n",
    "        \n",
    "        #Starting the stream!\n",
    "        source.start()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        '''\n",
    "        We can see that we have cuDF dataframe that got produced to the output. \n",
    "        Let's see if we can print some actual word counts.\n",
    "        ''' \n",
    "        print(output[-1].loc[9:])\n",
    "        \n",
    "        '''\n",
    "        We can! :)\n",
    "\n",
    "        Now, we write some more data to the text file and wait for some more time before checking the output again.\n",
    "\n",
    "        If we're sure of what's happening, the output should now have a list of cuDF dataframes, \n",
    "        each having the cumulative streaming word count of all the data seen until now, \n",
    "        the last cuDF dataframe being the most recent.\n",
    "        '''\n",
    "        #Write more random data to the file\n",
    "        for i in range(10,20):\n",
    "            f.write(\"this is line \" + str(i) + \"\\n\")\n",
    "        f.flush()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        print(output[-1].loc[9:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cudf_0.8)",
   "language": "python",
   "name": "cudf_0.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
