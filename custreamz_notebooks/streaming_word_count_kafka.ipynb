{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a cuStreamz job for the classic example of Streaming Word Count.**\n",
    "\n",
    "For this example, we will be demonstrating how to stream from Kafka. But one can also perform a streaming word count reading from a text file which is being continuously written into.\n",
    "\n",
    "You can refer to https://kafka.apache.org/quickstart to start a local Kafka cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streamz and cudf imports\n",
    "import cudf\n",
    "from streamz import Stream\n",
    "from streamz.dataframe import DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the data coming in to the Kafka topic â€” i.e., each record/message, is a line in the form of \"this is line x\", where x is an incremental counter. \n",
    "    \n",
    "Now, we write a function to parse each such message to get the list of words in each line. \n",
    "\n",
    "One can also make use of nvstrings (now custrings, the GPU-accelerated string manipulation library) to tokenise all the messages in the batch. Refer to process_batch_1()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function operating on every batch polled from Kafka for word count\n",
    "def process_batch(messages):\n",
    "    y = []\n",
    "    for x in messages:\n",
    "        y = y + x.decode('utf-8').strip('\\n').split(\" \")\n",
    "    return y\n",
    "\n",
    "import nvstrings, nvtext\n",
    "def process_batch_1(messages):\n",
    "    messages_1 = []\n",
    "    for message in messages:\n",
    "        messages_1.append(message.decode('utf-8'))\n",
    "    device_lines = nvstrings.to_device(messages_1)\n",
    "    words = nvtext.tokenize(device_lines)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Kafka consumer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kafka topic to read streaming data from\n",
    "topic = \"word-count\"\n",
    "\n",
    "#Kafka brokers\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "#Kafka consumer configuration\n",
    "consumer_conf = {'bootstrap.servers': bootstrap_servers, 'group.id': 'custreamz', 'session.timeout.ms': 60000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use Streamz to create a Stream from Kafka by polling the topic every 10s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you changed Dask=True, please ensure you have a Dask cluster up and running\n",
    "source = Stream.from_kafka_batched(topic, consumer_conf, npartitions=1, poll_interval='10s', asynchronous=True, dask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying process_batch function to process word count on each batch\n",
    "stream = source.map(process_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Streamz DataFrame does the trick!* \n",
    "\n",
    "After we get the parsed word list on our stream from Kafka, we just perform simple aggregations using the Streamz DataFrame to get the word count.\n",
    "\n",
    "We then write the output (word count) to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = stream.map(lambda words: cudf.DataFrame({'word': words, 'count': np.ones(len(words),dtype=np.int32)}))\n",
    "sdf = DataFrame(stream_df, example=cudf.DataFrame({'word':[], 'count':[]}))\n",
    "output = sdf.groupby('word').sum().stream.buffer(100000).gather().sink_to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the stream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what output we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<cudf.DataFrame ncols=1 nrows=70 >]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have cuDF dataframe that got produced to the output. Let's see if we can print some actual word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count\n",
      "65      1\n",
      "66      1\n",
      "7      1\n",
      "8      1\n",
      "9      1\n",
      "is     67\n",
      "line     67\n",
      "this     67\n"
     ]
    }
   ],
   "source": [
    "#Printing the values\n",
    "print(output[0].loc[65:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can! :) \n",
    "\n",
    "Now, let's wait for some more time before checking the output again. \n",
    "\n",
    "If we're sure of what's happening, the output should now have a list of cuDF dataframes, each having the cumulative streaming word count of all the data seen until now, the last cuDF dataframe being the most recent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<cudf.DataFrame ncols=1 nrows=70 >,\n",
       " <cudf.DataFrame ncols=1 nrows=185 >,\n",
       " <cudf.DataFrame ncols=1 nrows=292 >,\n",
       " <cudf.DataFrame ncols=1 nrows=400 >,\n",
       " <cudf.DataFrame ncols=1 nrows=507 >]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the output again\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count\n",
      "99      1\n",
      "is    504\n",
      "line    504\n",
      "this    504\n"
     ]
    }
   ],
   "source": [
    "#Printing the values again\n",
    "print(output[4].loc[99:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cudf_0.8)",
   "language": "python",
   "name": "cudf_0.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
