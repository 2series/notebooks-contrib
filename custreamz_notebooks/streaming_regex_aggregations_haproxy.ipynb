{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a cuStreamz job which streams HAProxy logs from Kafka, parses them, and performs some basic aggregations.**\n",
    "\n",
    "For this example, we will be demonstrating how to stream from Kafka. But one can also perform the same streaming operations reading from a text file which is being continuously written into.\n",
    "\n",
    "You can refer to: https://kafka.apache.org/quickstart to start a local Kafka cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*An example HAProxy log (JSON-encoded string as a message in Kafka) would be of the following form:*\n",
    "\n",
    "{\"logline\": \"[haproxy@10.0.0.1] <134>May 29 19:08:36 haproxy[113498]: 45.26.605.15:38738 [29/May/2019:19:08:36.691] HTTPS:443~ HTTP_ProvisionManagers/mp3 4/5/0/1/1 200 6182 - - --NI 3/3/0/0/0 0/0 {|} \"GET /v2/serverinfo HTTP/1.1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streamz and cudf imports\n",
    "from streamz import Stream\n",
    "import cudf\n",
    "from streamz.dataframe import DataFrame\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper function which implements RegEx parsing on the HAProxy logs, and then calculates the average backend response time for each batch.\n",
    "\n",
    "It also has timestamps to determine the time taken by each important phase of the stream processing — parsing and aggregations. These times are returned along with the average backend response time taken for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haproxy_regex_aggregations(messages):\n",
    "    \n",
    "    preprocess_start_time = int(round(time.time()))\n",
    "    \n",
    "    size = len(messages)*len(messages[0]) \n",
    "    num_rows = len(messages)\n",
    "    json_input_string = \"\\n\".join([msg.decode('utf-8') for msg in messages])\n",
    "    \n",
    "    gdf = cudf.read_json(json_input_string, lines=True, engine='cudf')\n",
    "    \n",
    "    pre_regex_time = int(round(time.time()))\n",
    "    \n",
    "    '''\n",
    "    Piecemeal log parsing for HAProxy\n",
    "    '''\n",
    "    \n",
    "    clean_df = gdf['logline'].str.split(' ')\n",
    "\n",
    "    clean_df['log_ip'] = clean_df[0].str.lstrip('[haproxy@').str.rstrip(']')\n",
    "    clean_df.drop_column(0)\n",
    "\n",
    "    clean_df[1] = clean_df[1].str.split('>')[1]\n",
    "    syslog_timestamp = clean_df[1].data.cat([clean_df[2].data, clean_df[3].data, clean_df[4].data], sep=' ')\n",
    "    clean_df['syslog_timestamp'] = cudf.Series(syslog_timestamp)\n",
    "    for col in [1,2,3,4]:\n",
    "        clean_df.drop_column(col)\n",
    "\n",
    "    program_pid_df = clean_df[5].str.split('[')\n",
    "    program_sr = program_pid_df[0]\n",
    "    pid_sr = program_pid_df[1]\n",
    "    clean_df['program'] = program_sr\n",
    "    clean_df['pid'] = pid_sr.str.rstrip(']:')\n",
    "    clean_df = clean_df.drop(labels=[5])\n",
    "    del program_pid_df\n",
    "\n",
    "    client_pid_port_df = clean_df[6].str.split(':')\n",
    "    clean_df['client_ip'], clean_df['client_port'] = client_pid_port_df[0], client_pid_port_df[1]\n",
    "    clean_df.drop_column(6)\n",
    "    del client_pid_port_df\n",
    "\n",
    "    clean_df['accept_date'] = clean_df[7].str.lstrip('[').str.rstrip(']')\n",
    "    clean_df.drop_column(7)\n",
    "\n",
    "    clean_df.rename({8: 'frontend_name'}, inplace=True)\n",
    "    backend_server_df = clean_df[9].str.split('/')\n",
    "    clean_df['backend_name'], clean_df['server_name'] = backend_server_df[0], backend_server_df[1]\n",
    "    clean_df.drop_column(9)\n",
    "\n",
    "    time_cols = ['time_request', 'time_queue', 'time_backend_connect', 'time_backend_response', 'time_duration']\n",
    "    time_df = clean_df[10].str.split('/')\n",
    "    for col_id, col_name in enumerate(time_cols):\n",
    "        clean_df[col_name] = time_df[col_id]\n",
    "    clean_df.drop_column(10)\n",
    "    del time_df\n",
    "\n",
    "    clean_df.rename({11: 'http_status_code'}, inplace=True)\n",
    "    clean_df.rename({12: 'bytes_read'}, inplace=True)\n",
    "    clean_df.rename({13: 'captured_request', 14: 'captured_response', 15: 'termination_state'}, inplace=True)\n",
    "\n",
    "    con_cols = ['actconn', 'feconn', 'beconn', 'srvconn', 'retries']\n",
    "    con_df = clean_df[16].str.split('/')\n",
    "    for col_id, col_name in enumerate(con_cols):\n",
    "        clean_df[col_name] = con_df[col_id]\n",
    "    clean_df.drop_column(16)\n",
    "    del con_df\n",
    "\n",
    "    q_df = clean_df[17].str.split('/')\n",
    "    clean_df['srv_queue'], clean_df['backend_queue'] = q_df[0], q_df[1]\n",
    "    clean_df.drop_column(17)\n",
    "    del q_df\n",
    "    \n",
    "    post_regex_time = int(round(time.time()))\n",
    "    \n",
    "    '''\n",
    "    End of the piecemeal Regex log parsing for HAProxy.\n",
    "    Simple aggregations to be performed now.\n",
    "    '''\n",
    "    \n",
    "    clean_df['time_backend_response'] = clean_df['time_backend_response'].astype('int')\n",
    "    avg_backend_response_time = clean_df['time_backend_response'].mean()\n",
    "    \n",
    "    post_agg_time = int(round(time.time()))\n",
    "    \n",
    "    return \"{0},{1},{2},{3},{4},{5},{6}\".format(num_rows, preprocess_start_time, pre_regex_time, \\\n",
    "                                            post_regex_time, post_agg_time, \\\n",
    "                                            avg_backend_response_time, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Kafka consumer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kafka topic to read streaming data from\n",
    "topic = \"haproxy-logs\"\n",
    "\n",
    "#Kafka brokers\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "#Kafka consumer configuration\n",
    "consumer_conf = {'bootstrap.servers': bootstrap_servers,\n",
    "                 'group.id': 'custreamz', 'session.timeout.ms': 60000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use Streamz to create a Stream from Kafka by polling the topic every 10s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you changed Dask=True, please ensure you have a Dask cluster up and running\n",
    "stream = Stream.from_kafka_batched(topic, consumer_conf, poll_interval='10s',\n",
    "                                   npartitions=1, asynchronous=True, dask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the helper regex+aggregations function to perform the required operations on each batch polled from Kafka, and write the result into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = stream.map(haproxy_regex_aggregations).buffer(100000).gather().sink_to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the stream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['57,1565208776,1565208777,1565208777,1565208777,5.543859649122807,10545',\n",
       " '125,1565208788,1565208788,1565208788,1565208788,5.512,23250']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After waiting for some more time, let's check the output again — the list should have grown, since more batches have been processed on the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['57,1565208776,1565208777,1565208777,1565208777,5.543859649122807,10545',\n",
       " '125,1565208788,1565208788,1565208788,1565208788,5.512,23250',\n",
       " '113,1565208800,1565208800,1565208800,1565208800,5.398230088495575,21244',\n",
       " '113,1565208811,1565208811,1565208811,1565208811,5.398230088495575,20905',\n",
       " '113,1565208823,1565208823,1565208823,1565208823,5.327433628318584,20905']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cudf_0.8)",
   "language": "python",
   "name": "cudf_0.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
