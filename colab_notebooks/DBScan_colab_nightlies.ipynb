{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRHktCLRGm1z"
   },
   "source": [
    "<h2> Density-Based Spatial Culstering of Applications with Noise (DBSCAN) </h2>\n",
    "<hr>\n",
    "\n",
    "<p>The DBSCAN algorithm is a clustering algorithm which works really well for datasets in which samples conregate in large groups. cuMLâ€™s DBSCAN expects a cuDF DataFrame, and constructs an adjacency graph to compute the distances between close neighbours. The DBSCAN model implemented in the cuML library can accept the following parameters :</p>\n",
    "<ol>\n",
    "  <li> eps: maximum distance between 2 sample points </li>\n",
    "  <li> min_samples: minimum number of samples that should be present in a neighborhood for it to be considered as a core points.</li>\n",
    "</ol>\n",
    "\n",
    "<p>The methods that can be used with DBSCAN are: </p>\n",
    "<ul>\n",
    "  <li>fit: Perform DBSCAN clustering from features.</li>\n",
    "  <li>fit_predict: Performs clustering on input_gdf and returns cluster labels.</li>\n",
    "  <li>get_params: Sklearn style return parameter state</li>\n",
    "  <li>set_params: Sklearn style set parameter state to dictionary of params.</li>\n",
    "  </ul>\n",
    " \n",
    "<p>The model accepts only numpy arrays or cudf dataframes as the input. In order to convert your dataset to cudf format please read the cudf documentation on https://rapidsai.github.io/projects/cudf/en/latest/. For additional information on the DBSCAN model please refer to the documentation on https://rapidsai.github.io/projects/cuml/en/latest/index.html </p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xsOMscit-OD"
   },
   "source": [
    " <h2> Setup </h2>\n",
    "\n",
    "<ol>\n",
    "  <li> Ensure that you have selected Python3 as your runtime type and 'GPU' as your hardware accelerator from the menu: Runtime > Change Runtime Type. </li> \n",
    "  <li>Use pynvml to confirm Colab allocated you a Tesla T4 GPU.</li>\n",
    "  <li> Install most recent Miniconda release compatible with Google Colab's Python install (3.6.7). </li>\n",
    "  <li> Install RAPIDS libraries. </li>\n",
    "  <li> Copy RAPIDS .so files into current working directory, a workaround for conda/colab interactions. </li>\n",
    "  <li> Update env variables so Python can find and use RAPIDS artifacts. </li>\n",
    "  <li> All of the above steps are automated in the next cell.\n",
    "  <li> You should re-run this cell any time your instance re-starts. </li>\n",
    "  \n",
    " </ol>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "colab_type": "code",
    "id": "sRtYd8jAtuE1",
    "outputId": "f197c933-b247-41c6-8d9c-7c15c97e9cee"
   },
   "outputs": [],
   "source": [
    "!wget -nc https://github.com/rapidsai/notebooks-extended/raw/master/utils/rapids-colab.sh\n",
    "!chmod +x rapids-colab.sh\n",
    "!./rapids-colab.sh\n",
    "\n",
    "import sys, os\n",
    "\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
    "\n",
    "import nvstrings, nvcategory, cudf, cuml, xgboost\n",
    "import dask_cudf, dask_cuml\n",
    "\n",
    "!mkdir data\n",
    "!wget https://github.com/rapidsai/notebooks/raw/branch-0.8/cuml/data/mortgage.npy.gz -O data/mortgage.npy.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XCa8PU_vKci"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN as skDBSCAN\n",
    "from cuml import DBSCAN as cumlDBSCAN\n",
    "import cudf\n",
    "import os, gzip\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FB45vOmTNBJF"
   },
   "source": [
    " <h2> Data </h2>\n",
    "\n",
    "<p>Here we can utilize either of the two load data functions: </p>\n",
    "<ol>\n",
    "  <li> Loading data from the zipped file into regular dataframe.</li>\n",
    "  <li> Loading data from the zipped file into a CUDA dataframe.\n",
    "    \n",
    "<p> NOTE: The following functions both provide the same end result (a pandas dataframe). </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VApzmf9pvbNh"
   },
   "outputs": [],
   "source": [
    "def load_data(nrows, ncols, cached = 'data/mortgage.npy.gz'):\n",
    "  if os.path.exists(cached):\n",
    "      print('Using Mortgage Data')\n",
    "      with gzip.open(cached) as f:\n",
    "          X = np.load(f)\n",
    "      X = X[np.random.randint(0,X.shape[0]-1,nrows),:ncols]\n",
    "  else:\n",
    "      # create a random dataset\n",
    "      print('Using Random Data')\n",
    "      X = np.random.rand(nrows,ncols)\n",
    "  df = pd.DataFrame({'fea%d'%i:X[:,i] for i in range(X.shape[1])})\n",
    "  return df\n",
    "\n",
    "def load_data_alternate(nrows, ncols, cached = 'data/mortgage.csv.gz'):\n",
    "  if os.path.exists(cached):\n",
    "    with gzip.open(cached) as f:\n",
    "      print('Using Mortgage Data')\n",
    "      X = cudf.read_csv(f, usecols=[i for i in range(0, ncols)], nrows=nrows, header=None)\n",
    "      return X\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_iOGAARKIgR4",
    "outputId": "6ef76b35-0934-4614-d048-02ef069d9d0b"
   },
   "outputs": [],
   "source": [
    "# Setting the  number of rows and columns that will be imported.\n",
    "# Play around with the numbers: let nrows be (500, 5000) and run tests.\n",
    "nrows = 10000\n",
    "ncols = 20\n",
    "df = load_data(nrows, ncols)\n",
    "#df = load_data_alternate(nrows, ncols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdGAVt_v754j"
   },
   "source": [
    "<h2> Performing Clustering </h2>\n",
    "\n",
    "<p> Setting up variables for distance between 2 sample points and the minimum number of samples for the DBSCAN algorithm.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQRpLFzP8LTY"
   },
   "outputs": [],
   "source": [
    "# eps = maximum distance between 2 sample points for them to be in the same neighborhood\n",
    "# min_samples = number of samples that should be present in a neighborhood for it to be considered as a core point\n",
    "\n",
    "eps = 3\n",
    "min_samples = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "777wCl15-GiF"
   },
   "source": [
    "**<p> At this point, we can now compare the performance between the traditional sklearn dbscan model and the implementation done utilizing CUDA. </p>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_MFt-o478Mhr",
    "outputId": "8797d3a5-f420-4fca-9aa6-d81af7574010"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# use the sklearn DBSCAN model to fit the dataset \n",
    "clustering_sk = skDBSCAN(eps = eps, min_samples = min_samples)\n",
    "clustering_sk.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqd-U-wDpVJP"
   },
   "outputs": [],
   "source": [
    "df = cudf.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iXi6SI3y8Mvu",
    "outputId": "318ea473-0302-43a2-bb32-522058f728e3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# run the cuml DBSCAN model to fit the dataset \n",
    "clustering_cuml = cumlDBSCAN(eps = eps, min_samples = min_samples)\n",
    "clustering_cuml.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8USwaCjrt2A"
   },
   "source": [
    "**<p>These two functions determine whether the results from cuml and sklearn are equivalent.</p>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0ovJ2j-rcKn"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# the function converts a variable from ndarray or dataframe format to numpy array\n",
    "def to_nparray(x):\n",
    "    if isinstance(x,np.ndarray) or isinstance(x,pd.DataFrame):\n",
    "        return np.array(x)\n",
    "    elif isinstance(x,np.float64):\n",
    "        return np.array([x])\n",
    "    elif isinstance(x,cudf.DataFrame) or isinstance(x,cudf.Series):\n",
    "        return x.to_pandas().values\n",
    "    return x\n",
    "\n",
    "def array_equal(a,b,threshold=5e-3,with_sign=True):\n",
    "    a = to_nparray(a)\n",
    "    b = to_nparray(b)\n",
    "    if with_sign == False:\n",
    "        a,b = np.abs(a),np.abs(b)\n",
    "    error = mean_squared_error(a,b)\n",
    "    res = error<threshold\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKD8-SADmjP_"
   },
   "source": [
    "**<p>Ensuring that the results from both methods give the same output.</p>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Nqv9d1B8mi4B",
    "outputId": "50f806cc-ed74-4bc1-8747-1e9bc704b591"
   },
   "outputs": [],
   "source": [
    "equals = array_equal(clustering_cuml.labels_,clustering_sk.labels_)\n",
    "if equals:\n",
    "  print(\"Results are equal.\")\n",
    "else:\n",
    "  print(\"Results are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSmHNc-Fu0QU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DBScan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
