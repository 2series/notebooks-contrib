{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DBScan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NRHktCLRGm1z"
      },
      "source": [
        "<h2> Density-Based Spatial Culstering of Applications with Noise (DBSCAN) </h2>\n",
        "<hr>\n",
        "\n",
        "<p>The DBSCAN algorithm is a clustering algorithm which works really well for datasets in which samples conregate in large groups. cuMLâ€™s DBSCAN expects a cuDF DataFrame, and constructs an adjacency graph to compute the distances between close neighbours. The DBSCAN model implemented in the cuML library can accept the following parameters :</p>\n",
        "<ol>\n",
        "  <li> eps: maximum distance between 2 sample points </li>\n",
        "  <li> min_samples: minimum number of samples that should be present in a neighborhood for it to be considered as a core points.</li>\n",
        "</ol>\n",
        "\n",
        "<p>The methods that can be used with DBSCAN are: </p>\n",
        "<ul>\n",
        "  <li>fit: Perform DBSCAN clustering from features.</li>\n",
        "  <li>fit_predict: Performs clustering on input_gdf and returns cluster labels.</li>\n",
        "  <li>get_params: Sklearn style return parameter state</li>\n",
        "  <li>set_params: Sklearn style set parameter state to dictionary of params.</li>\n",
        "  </ul>\n",
        " \n",
        "<p>The model accepts only numpy arrays or cudf dataframes as the input. In order to convert your dataset to cudf format please read the cudf documentation on https://rapidsai.github.io/projects/cudf/en/latest/. For additional information on the DBSCAN model please refer to the documentation on https://rapidsai.github.io/projects/cuml/en/latest/index.html </p>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7xsOMscit-OD"
      },
      "source": [
        " <h2> Setup </h2>\n",
        "\n",
        "<ol>\n",
        "  <li> Ensure that you have selected Python3 as your runtime type and 'GPU' as your hardware accelerator from the menu: Runtime > Change Runtime Type. </li> \n",
        "  <li>Use pynvml to confirm Colab allocated you a Tesla T4 GPU.</li>\n",
        "  <li> Install most recent Miniconda release compatible with Google Colab's Python install (3.6.7). </li>\n",
        "  <li> Install RAPIDS libraries. </li>\n",
        "  <li> Copy RAPIDS .so files into current working directory, a workaround for conda/colab interactions. </li>\n",
        "  <li> Update env variables so Python can find and use RAPIDS artifacts. </li>\n",
        "  <li> All of the above steps are automated in the next cell.\n",
        "  <li> You should re-run this cell any time your instance re-starts. </li>\n",
        "  \n",
        " </ol>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sRtYd8jAtuE1",
        "colab": {}
      },
      "source": [
        "\"\"\"install RAPIDS AI suite and set up environment\n",
        ">> may take a few minutes, long output (output display removed)\n",
        "\"\"\"\n",
        "!wget -nc https://github.com/rapidsai/notebooks-extended/raw/master/utils/rapids-colab.sh\n",
        "!bash rapids-colab.sh\n",
        "\n",
        "import sys, os\n",
        "\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "\n",
        "\n",
        "\"\"\"imports\n",
        "\"\"\"\n",
        "import gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# rapids\n",
        "import cudf\n",
        "from cuml import DBSCAN as cumlDBSCAN\n",
        "from sklearn.cluster import DBSCAN as skDBSCAN\n",
        "# dask\n",
        "import dask\n",
        "import dask.dataframe as dd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FB45vOmTNBJF"
      },
      "source": [
        " <h2> Data </h2>\n",
        "\n",
        "<p>Here we can utilize either of the two load data functions: </p>\n",
        "<ol>\n",
        "  <li> Loading data from the zipped file into regular dataframe.</li>\n",
        "  <li> Loading data from the zipped file into a CUDA dataframe.\n",
        "    \n",
        "<p> NOTE: The following functions both provide the same end result (a pandas dataframe). </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VApzmf9pvbNh",
        "colab": {}
      },
      "source": [
        "def load_data(nrows, ncols, cached = 'data/mortgage.npy.gz'):\n",
        "  if os.path.exists(cached):\n",
        "      print('Using Mortgage Data')\n",
        "      with gzip.open(cached) as f:\n",
        "          X = np.load(f)\n",
        "      X = X[np.random.randint(0,X.shape[0]-1,nrows),:ncols]\n",
        "  else:\n",
        "      # create a random dataset\n",
        "      print('Using Random Data')\n",
        "      X = np.random.rand(nrows,ncols)\n",
        "  df = pd.DataFrame({'fea%d'%i:X[:,i] for i in range(X.shape[1])})\n",
        "  return df\n",
        "\n",
        "def load_data_alternate(nrows, ncols, cached = 'data/mortgage.csv.gz'):\n",
        "  if os.path.exists(cached):\n",
        "    with gzip.open(cached) as f:\n",
        "      print('Using Mortgage Data')\n",
        "      X = cudf.read_csv(f, usecols=[i for i in range(0, ncols)], nrows=nrows, header=None)\n",
        "      return X\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_iOGAARKIgR4",
        "outputId": "9d130ed2-f5a5-4d62-fa0a-a8cb32b874eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Setting the  number of rows and columns that will be imported.\n",
        "# Play around with the numbers: let nrows be (500, 5000) and run tests.\n",
        "nrows = 10000\n",
        "ncols = 20\n",
        "df = load_data(nrows, ncols)\n",
        "#df = load_data_alternate(nrows, ncols)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Random Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jdGAVt_v754j"
      },
      "source": [
        "<h2> Performing Clustering </h2>\n",
        "\n",
        "<p> Setting up variables for distance between 2 sample points and the minimum number of samples for the DBSCAN algorithm.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BQRpLFzP8LTY",
        "colab": {}
      },
      "source": [
        "# eps = maximum distance between 2 sample points for them to be in the same neighborhood\n",
        "# min_samples = number of samples that should be present in a neighborhood for it to be considered as a core point\n",
        "\n",
        "eps = 3\n",
        "min_samples = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "777wCl15-GiF"
      },
      "source": [
        "**<p> At this point, we can now compare the performance between the traditional sklearn dbscan model and the implementation done utilizing CUDA. </p>**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_MFt-o478Mhr",
        "outputId": "70bd53d8-b5ab-477c-d401-a47522db4a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# use the sklearn DBSCAN model to fit the dataset \n",
        "clustering_sk = skDBSCAN(eps = eps, min_samples = min_samples)\n",
        "clustering_sk.fit(df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.72 s, sys: 1.29 s, total: 7.02 s\n",
            "Wall time: 7.04 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aqd-U-wDpVJP",
        "colab": {}
      },
      "source": [
        "# convert dataframe to cudf from pandas \n",
        "df = cudf.from_pandas(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iXi6SI3y8Mvu",
        "outputId": "8fdd69e5-7a4b-43e2-c129-2e6a0af97885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# run the cuml DBSCAN model to fit the dataset \n",
        "clustering_cuml = cumlDBSCAN(eps = eps, min_samples = min_samples)\n",
        "clustering_cuml.fit(df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.1 s, sys: 124 ms, total: 1.22 s\n",
            "Wall time: 1.22 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v8USwaCjrt2A"
      },
      "source": [
        "**<p>These two functions determine whether the results from cuml and sklearn are equivalent.</p>**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0ovJ2j-rcKn",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# the function converts a variable from ndarray or dataframe format to numpy array\n",
        "def to_nparray(x):\n",
        "    if isinstance(x,np.ndarray) or isinstance(x,pd.DataFrame):\n",
        "        return np.array(x)\n",
        "    elif isinstance(x,np.float64):\n",
        "        return np.array([x])\n",
        "    elif isinstance(x,cudf.DataFrame) or isinstance(x,cudf.Series):\n",
        "        return x.to_pandas().values\n",
        "    return x\n",
        "\n",
        "def array_equal(a,b,threshold=5e-3,with_sign=True):\n",
        "    a = to_nparray(a)\n",
        "    b = to_nparray(b)\n",
        "    if with_sign == False:\n",
        "        a,b = np.abs(a),np.abs(b)\n",
        "    error = mean_squared_error(a,b)\n",
        "    res = error<threshold\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKD8-SADmjP_"
      },
      "source": [
        "**<p>Ensuring that the results from both methods give the same output.</p>**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nqv9d1B8mi4B",
        "outputId": "639950f7-aa21-41d9-fca4-54afa4e38299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "equals = array_equal(clustering_cuml.labels_,clustering_sk.labels_)\n",
        "if equals:\n",
        "  print(\"Results are equal.\")\n",
        "else:\n",
        "  print(\"Results are not equal.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results are equal.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCR9M6xXJXrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}