{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyber Use Case Tutorial #2: Network Mapping using RAPIDS\n",
    "\n",
    "### GTC SJ 2019 (18 March 2019)\n",
    "### Authors:\n",
    "- Bianca Rhodes (NVIDIA)\n",
    "- Bartley Richardson (NVIDIA)\n",
    "- Eli Fajardo (NVIDIA)\n",
    "- Bhargav Suryadevara (NVIDIA)\n",
    "- Nick Becker (NVIDIA)\n",
    "\n",
    "### Goals:\n",
    "- Parse raw Windows Event Logs using cuDF\n",
    "- Load netflow data into a cuDF\n",
    "- Map parsed data to network graph edges using cuDF\n",
    "- Use cuGraph pagerank\n",
    "- Build a network graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import dask_cudf\n",
    "import dask.delayed\n",
    "import nvstrings\n",
    "import nvcategory\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import cugraph\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import cudf\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait\n",
    "from collections import OrderedDict\n",
    "from cudf.core import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "With the insurmountable flow of data and connected devices today, it becomes critical to be able to map that data into a network graph for easy visual reference and analytics. We strive to recognize patterns and anomolies to combat cyber attacks.  \n",
    "  \n",
    "One of the common struggles today is the ability to parse data with speed. Here we will demonstrate how to parse raw Windows Event Logs.\n",
    "  \n",
    "By the end of this tutorial, we'll be able to parse raw Windows Event Logs containing authorization data, combine with netflow data, to form a network mapping graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Windows Event Logs\n",
    "\n",
    "In the cell below we'll be setting variables for the input columns and output columns.\n",
    "\n",
    "First, define the input columns and dtypes. These input columns are defined by the data source provided by [Los Alamos National Laboratory](https://csr.lanl.gov/data/2017.html). The additional column \"Raw\" integrates the values from those columns to form a raw Windows Event Log.\n",
    "  \n",
    "Next, define the output columns and dtypes. These output columns are defined by the content of the Windows Event logs and more directly defined by the configuration of regex values `conf/lanl_regex_configs` used to parse each key value pair from the raw log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLS_SET = ['Time',\n",
    "                  'EventID',\n",
    "                  'LogHost',\n",
    "                  'LogonType',\n",
    "                  'LogonTypeDescription',\n",
    "                  'UserName',\n",
    "                  'DomainName',\n",
    "                  'LogonID',\n",
    "                  'SubjectUserName',\n",
    "                  'SubjectDomainName',\n",
    "                  'SubjectLogonID',\n",
    "                  'Status',\n",
    "                  'Source',\n",
    "                  'ServiceName',\n",
    "                  'Destination',\n",
    "                  'AuthenticationPackage',\n",
    "                  'FailureReason',\n",
    "                  'ProcessName',\n",
    "                  'ProcessID',\n",
    "                  'ParentProcessName',\n",
    "                  'ParentProcessID',\n",
    "                  'Raw']\n",
    "INPUT_DTYPES = ['str' for x in INPUT_COLS_SET]\n",
    "\n",
    "OUTPUT_COLS_SUPERSET = ['detailed_authentication_information_authentication_package',\n",
    "                        'new_logon_logon_guid',\n",
    "                        'failure_information_failure_reason',\n",
    "                        'failure_information_status',\n",
    "                        'computername',\n",
    "                        'new_logon_logon_id',\n",
    "                        'subject_security_id',\n",
    "                        'detailed_authentication_information_package_name_ntlm_only',\n",
    "                        'logon_type',\n",
    "                        'account_for_which_logon_failed_security_id',\n",
    "                        'detailed_authentication_information_key_length',\n",
    "                        'subject_logon_id',\n",
    "                        'process_information_caller_process_name',\n",
    "                        'eventcode',\n",
    "                        'process_information_caller_process_id',\n",
    "                        'subject_account_name',\n",
    "                        'process_information_process_name',\n",
    "                        'new_logon_account_name',\n",
    "                        'process_information_process_id',\n",
    "                        'failure_information_sub_status',\n",
    "                        'new_logon_security_id',\n",
    "                        'network_information_source_network_address',\n",
    "                        'detailed_authentication_information_transited_services',\n",
    "                        'new_logon_account_domain',\n",
    "                        'subject_account_domain',\n",
    "                        'detailed_authentication_information_logon_process',\n",
    "                        'account_for_which_logon_failed_account_domain',\n",
    "                        'account_for_which_logon_failed_account_name',\n",
    "                        'network_information_workstation_name',\n",
    "                        'network_information_source_port',\n",
    "                        'subject_security_id']\n",
    "OUTPUT_DTYPES = ['float64' for x in OUTPUT_COLS_SUPERSET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Data Pipeline\n",
    "\n",
    "Preprocess, or clean the raw logs, by removing the non-printable characters then begin to parse logs by event code type. Here we use regex mappings per event code to parse out each data element defined in OUTPUT_COLS_SUPERSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, event_codes, regex_mappings, clean=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if clean:\n",
    "        df = preprocess_logs(df)\n",
    "    out_dfs = [] \n",
    "    \n",
    "    # separate by eventcode and process differently\n",
    "    \n",
    "    for code in event_codes:\n",
    "        portion = filter_by_pattern(df, code)\n",
    "        temp = process_logtype(portion, regex_mappings, code)\n",
    "        temp['time'] = portion['Time'].astype('int')\n",
    "        temp['eventcode'] = portion['EventID']\n",
    "        out_dfs.append(temp)\n",
    "        \n",
    "    # recombine the processed output\n",
    "    out_df = cudf.concat(out_dfs)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def concat_wrapper(df_list):\n",
    "    return cudf.concat(df_list)\n",
    "\n",
    "\n",
    "def preprocess_logs(logs_gdf):\n",
    "    \"\"\"Lowercasing and replacing characters\n",
    "    \"\"\"\n",
    "    logs_gdf['Raw'] = (\n",
    "        logs_gdf['Raw'].str.lower()\n",
    "        .str.replace('\\\\\\\\t', '')\n",
    "        .str.replace('\\\\\\\\r', '')\n",
    "        .str.replace('\\\\\\\\n', ' | ')\n",
    "    )\n",
    "    return logs_gdf\n",
    "\n",
    "\n",
    "def process_logtype(df, regexes, eventcode):\n",
    "    \"\"\"Ongoing strings development/fixes will allow for cleaner log processing code in the future\n",
    "    \"\"\"\n",
    "\n",
    "    log_df_processed = cudf.read_csv('conf/LANL_OUTPUT_COLS_SUPERSET.csv', dtype=OUTPUT_DTYPES)\n",
    "    \n",
    "    log_df_processed = log_df_processed[:0]\n",
    "    columns = list(regexes[eventcode].keys())\n",
    "\n",
    "    for col in columns:\n",
    "        regex_pattern = regexes[eventcode].get(col)\n",
    "        extracted_nvstrings = df['Raw'].str.extract(regex_pattern)\n",
    "        \n",
    "        if not extracted_nvstrings.empty:\n",
    "            log_df_processed[col] = extracted_nvstrings[0]\n",
    "    \n",
    "    for col in log_df_processed.columns:\n",
    "        if not log_df_processed[col].empty:\n",
    "            if log_df_processed[col].dtype == 'float64':\n",
    "                log_df_processed[col] = log_df_processed[col].astype('int').astype('str')\n",
    "            elif log_df_processed[col].dtype == 'object':\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                log_df_processed[col] = log_df_processed[col].astype('str')\n",
    "        \n",
    "        if log_df_processed[col].empty:\n",
    "            log_df_processed[col] = nvstrings.to_device([])\n",
    "    \n",
    "    return log_df_processed\n",
    "\n",
    "\n",
    "def filter_by_pattern(df, pattern):\n",
    "    \"\"\"Filter based on whether a string contains a reex pattern\n",
    "    \"\"\"\n",
    "    df['present'] = df['EventID'].str.contains(pattern)\n",
    "    return df[df.present == True]\n",
    "\n",
    "\n",
    "def read_data(filename, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    gdf = dask_cudf.read_csv(filename, **kwargs)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def load_regex_yaml(yaml_file):\n",
    "    with open(yaml_file) as f:\n",
    "        regex_dict = yaml.safe_load(f)\n",
    "        regex_dict = {k: v[0] for k, v in regex_dict.items()}\n",
    "    return regex_dict\n",
    "\n",
    "\n",
    "def create_regex_dictionaries(yaml_directory):\n",
    "    regex_dict = {}\n",
    "    for f in os.listdir(yaml_directory):\n",
    "        if(os.path.splitext(f)[1] == '.yaml'):,"
    "             temp_regex = load_regex_yaml(yaml_directory + '/' + f)\n",
    "             regex_dict[f[:-5]] = temp_regex\n",
    "        \n",
    "    return regex_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Parsing Data Pipeline\n",
    "\n",
    "In this instance, we'll be focusing on parsing raw Windows Event Logs that are of event code [4624](https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/event.aspx?eventid=4624) and [4625](https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/event.aspx?eventid=4625)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../../../data/input/lanl\n",
    "!if [ ! -f ../../../data/input/lanl/wls.csv ]; then tar -xzvf ../../../data/lanl/wls.tar.gz -C ../../../data/input/lanl; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>EventID</th>\n",
       "      <th>Raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Time</td>\n",
       "      <td>EventID</td>\n",
       "      <td>Raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118781</td>\n",
       "      <td>4624</td>\n",
       "      <td>'An account was successfully logged on.\\n\\nSub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118781</td>\n",
       "      <td>4624</td>\n",
       "      <td>'An account was successfully logged on.\\n\\nSub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118781</td>\n",
       "      <td>4624</td>\n",
       "      <td>'An account was successfully logged on.\\n\\nSub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118781</td>\n",
       "      <td>4624</td>\n",
       "      <td>'An account was successfully logged on.\\n\\nSub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time  EventID                                                Raw\n",
       "0    Time  EventID                                                Raw\n",
       "1  118781     4624  'An account was successfully logged on.\\n\\nSub...\n",
       "2  118781     4624  'An account was successfully logged on.\\n\\nSub...\n",
       "3  118781     4624  'An account was successfully logged on.\\n\\nSub...\n",
       "4  118781     4624  'An account was successfully logged on.\\n\\nSub..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw lanl data parsing.\n",
    "AUTH_INPUT_PATH = '../../../data/input/lanl/wls.csv'\n",
    "REGEX_CONF_PATH = 'conf/lanl_regex_configs'\n",
    "EVENT_CODES_OF_INTEREST = ['4624','4625']\n",
    "REQUIRED_COLS = ['Time','EventID','Raw']\n",
    "DELIMITER = ','\n",
    "\n",
    "logs_gddf = dask_cudf.read_csv(AUTH_INPUT_PATH,\n",
    "                               names=INPUT_COLS_SET,\n",
    "                               delimiter=DELIMITER,\n",
    "                               usecols=REQUIRED_COLS,\n",
    "                               dtype=INPUT_DTYPES,\n",
    "                               skip_blank_lines=True,\n",
    "                              )\n",
    "logs_gddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time       object\n",
       "EventID    object\n",
       "Raw        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_gddf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pipeline() missing 1 required positional argument: 'regex_mappings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-53245a8b3771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#parts = [dask.delayed(pipeline)(x, EVENT_CODES_OF_INTEREST, REGEX_MAPPINGS) for x in logs_gddf.to_delayed()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEVENT_CODES_OF_INTEREST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREGEX_MAPPINGS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs_gddf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_delayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask_cudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_delayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# Bring data back to a single GPU, for downstream graph analytics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mauth_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask/dataframe/io/io.py\u001b[0m in \u001b[0;36mfrom_delayed\u001b[0;34m(dfs, meta, divisions, prefix, verify_meta)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mget_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_thread_get_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mpack_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpack_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m                         \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Re-execute locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                         \u001b[0mraise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cache\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask/local.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask/local.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/dask/core.py\u001b[0m in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0margs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mishashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: pipeline() missing 1 required positional argument: 'regex_mappings'"
     ]
    }
   ],
   "source": [
    "#raw lanl data parsing.\n",
    "AUTH_INPUT_PATH = '../../../data/input/lanl/wls.csv'\n",
    "REGEX_CONF_PATH = 'conf/lanl_regex_configs'\n",
    "EVENT_CODES_OF_INTEREST = ['4624','4625']\n",
    "REQUIRED_COLS = ['Time','EventID','Raw']\n",
    "DELIMITER = ','\n",
    "\n",
    "logs_gddf = dask_cudf.read_csv(AUTH_INPUT_PATH,\n",
    "                               names=INPUT_COLS_SET,\n",
    "                               delimiter=DELIMITER,\n",
    "                               usecols=REQUIRED_COLS,\n",
    "                               dtype=INPUT_DTYPES,\n",
    "                               skip_blank_lines=True,\n",
    "                              )\n",
    "\n",
    "REGEX_MAPPINGS = create_regex_dictionaries(REGEX_CONF_PATH)\n",
    "\n",
    "#parts = [dask.delayed(pipeline)(x, EVENT_CODES_OF_INTEREST, REGEX_MAPPINGS) for x in logs_gddf.to_delayed()]\n",
    "parts = [dask.delayed(pipeline)(x, EVENT_CODES_OF_INTEREST, REGEX_MAPPINGS) for x in logs_gddf.to_delayed()]\n",
    "temp_df = dask_cudf.from_delayed(parts)\n",
    "# Bring data back to a single GPU, for downstream graph analytics\n",
    "auth_gdf = temp_df.compute()\n",
    "print(auth_gdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read edge definitions from JSON file\n",
    "\n",
    "Now the parsing of Windows Event Logs has concluded, next we prepare for the network mapping portion. Within the edge definitions configuration file we define our edges by indicating the source and destination for each edge, referencing the column names of our input data.  \n",
    "  \n",
    "Below we also read in the netflow data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'conf/edge-definitions.json'\n",
    "with open(filename) as f:\n",
    "    edge_defs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network mapping edge list\n",
    "\n",
    "This function helps to recognize the data types we read in via csv, particulary the string objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtypes(fn, delim, floats, strings):\n",
    "    with open(fn, errors='replace') as fp:\n",
    "        header = fp.readline().strip()\n",
    "    \n",
    "    types = []\n",
    "    for col in header.split(delim):\n",
    "        if 'date' in col:\n",
    "            types.append((col, 'date'))\n",
    "        elif col in floats:\n",
    "            types.append((col, 'float64'))\n",
    "        elif col in strings:\n",
    "            types.append((col, 'str'))\n",
    "        else:\n",
    "            types.append((col, 'int32'))\n",
    "\n",
    "    return OrderedDict(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Netflow Data\n",
    "\n",
    "The netflow data is also provided by [Los Alamos National Laboratory](https://csr.lanl.gov/data/2017.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../../../data/input/lanl\n",
    "!if [ ! -f ../../../data/input/lanl/netflow.csv ]; then tar -xzvf ../../../data/lanl/netflow.tar.gz -C ../../../data/input/lanl; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_input_path = '../../../data/input/lanl/netflow.csv'\n",
    "dtypes_data_processed = get_dtypes(flow_input_path, ',', floats=[], strings=[\"SrcDevice\", \"DstDevice\"])   \n",
    "flow_gdf = cudf.io.csv.read_csv(flow_input_path, delimiter=',', names=list(dtypes_data_processed), \n",
    "                                       dtype=list(dtypes_data_processed.values()), skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to reference both the auth data (parsed Windows Event Logs) and netflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gdfs = {\n",
    "           'lanl_auth': auth_gdf, \n",
    "           'lanl_flow': flow_gdf\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build edges dataframe\n",
    "\n",
    "In the cell below, reference each datasource and its corresponding edge configuration to build a new dataframe containing edges. This dataframe will notably contain `srcCol` and `dstCol` along with other reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edge_defs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3ea9fd3990a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0medges_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medge_defs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mds_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_gdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataSource'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'edge_defs' is not defined"
     ]
    }
   ],
   "source": [
    "edges_gdf = None\n",
    "\n",
    "for ds in edge_defs:\n",
    "    \n",
    "    ds_gdf = ds_gdfs[ds['dataSource']]\n",
    "    \n",
    "    for e in ds[\"edges\"]:\n",
    "        \n",
    "        evtCols = ds[\"stringCols\"].copy()\n",
    "        \n",
    "        evtCols.append(e[\"srcCol\"])\n",
    "        evtCols.append(e[\"dstCol\"])\n",
    "        evtCols.append(ds[\"timeCol\"])\n",
    "        if 'filters' in e:\n",
    "            for f in e['filters']:\n",
    "                evtCols.append(f['key'])\n",
    "        evtCols = list(set(evtCols))\n",
    "        eventsDF = ds_gdf\n",
    "        eventsDF = eventsDF[evtCols]\n",
    "        \n",
    "        # Apply filters indicated in the edge configuration file\n",
    "        if 'filters' in e:\n",
    "            for f in e['filters']:\n",
    "                eventsDF = eventsDF[eventsDF[f['key']].str.contains(f['value']) == True]\n",
    "        \n",
    "        # Remove any None values\n",
    "        src_idx = eventsDF[e['srcCol']].str.contains(\"None\")\n",
    "        if len(eventsDF[src_idx])>0:\n",
    "            eventsDF = eventsDF[src_idx==False]\n",
    "        \n",
    "        dst_idx = eventsDF[e['dstCol']].str.contains(\"None\")\n",
    "        if len(eventsDF[dst_idx])>0:\n",
    "            eventsDF = eventsDF[dst_idx==False]        \n",
    "                \n",
    "        evt_edges_gdf = cudf.DataFrame()\n",
    "        evt_edges_gdf['src'] = eventsDF[e[\"srcCol\"]]\n",
    "        evt_edges_gdf['dst'] = eventsDF[e[\"dstCol\"]]\n",
    "        \n",
    "        # Adjust time to recent date (LANL data source begins at 1 second)\n",
    "        evt_edges_gdf['time'] = eventsDF[ds[\"timeCol\"]]+1442131200\n",
    "        \n",
    "        evt_edges_gdf['src_node_type'] = e[\"srcNodeType\"]\n",
    "        evt_edges_gdf['dst_node_type'] = e[\"dstNodeType\"]\n",
    "        evt_edges_gdf['relationship'] = e[\"relationship\"]\n",
    "        evt_edges_gdf['data_source'] = ds[\"dataSource\"]\n",
    "        \n",
    "        if edges_gdf is None:\n",
    "            edges_gdf = evt_edges_gdf\n",
    "        else:\n",
    "            edges_gdf = cudf.concat([edges_gdf, evt_edges_gdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas to drop duplicates as this is not yet available in cudf for strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6d76c53bf44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0medges_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0medges_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_pd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_pandas'"
     ]
    }
   ],
   "source": [
    "edges_pd = edges_gdf.to_pandas().drop_duplicates()\n",
    "edges_gdf = cudf.DataFrame.from_pandas(edges_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create node list and assign numeric ids\n",
    "\n",
    "Now that we have `edges_gdf` we can prepare the data for cuGraph by assigning continguous ids to the nodes and edges. cuGraph requires that all edges and nodes be identified using contiguous IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_nodes_pd = edges_pd[['src', 'src_node_type']].rename(columns={\"src\": \"id\", \"src_node_type\": \"node_type\"}).drop_duplicates()\n",
    "dst_nodes = edges_pd[['dst', 'dst_node_type']].rename(columns={\"dst\": \"id\", \"dst_node_type\": \"node_type\"}).drop_duplicates()\n",
    "all_nodes_pd = src_nodes_pd.append(dst_nodes).drop_duplicates()\n",
    "all_nodes_gdf = cudf.DataFrame.from_pandas(all_nodes_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign contiguous id's to nodes for cugraph\n",
    "idx = np.arange(len(all_nodes_gdf))\n",
    "all_nodes_gdf['idx'] = idx\n",
    "idmap_gdf = cudf.DataFrame({'id': all_nodes_gdf['id'], 'idx': idx})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add numeric src and dst node ids to edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add contiguous src id's to edges\n",
    "edges_gdf['id'] = edges_gdf['src']\n",
    "edges_gdf = edges_gdf.merge(idmap_gdf, on=['id'])\n",
    "edges_gdf['src_idx'] = edges_gdf['idx']\n",
    "edges_gdf = edges_gdf.drop(['id', 'idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add contiguous dst id's to edges\n",
    "edges_gdf['id'] = edges_gdf['dst']\n",
    "edges_gdf = edges_gdf.merge(idmap_gdf, on=['id'])\n",
    "edges_gdf['dst_idx'] = edges_gdf['idx']\n",
    "edges_gdf = edges_gdf.drop(['id', 'idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input edge list for cuGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_edges_gdf = edges_gdf[['src_idx', 'dst_idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_edges_gdf['src_idx'] = cg_edges_gdf['src_idx'].astype('int32')\n",
    "cg_edges_gdf['dst_idx'] = cg_edges_gdf['dst_idx'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cuGraph PageRank\n",
    "\n",
    "Next we create our graph and run pagerank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cugraph.Graph()\n",
    "G.add_edge_list(cg_edges_gdf['src_idx'], cg_edges_gdf['dst_idx'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pr_gdf = cugraph.pagerank(G, alpha=0.85, max_iter=500, tol=1.0e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add PageRank scores to node list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_gdf['idx'] = pr_gdf['vertex'].astype('int64')\n",
    "all_nodes_gdf = all_nodes_gdf.merge(pr_gdf, on=['idx'], how='left')\n",
    "all_nodes_gdf = all_nodes_gdf.drop(['vertex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphistry Viz\n",
    "\n",
    "We use [Graphistry](https://www.graphistry.com/) to visualize the network mapping graph. A snapshot of the graph constructed from this notebook can be viewed below. Run the code below to contruct a graph using Graphistry.\n",
    "\n",
    "A snapshot of the graph constructed from this notebook is provided below. To generate it yourself, you'll need to register an account with Graphistry and configure your key below.\n",
    "\n",
    "  ![Network Mapping Graphistry Image 1](images/graphistry1.png \"Network Mapping using Graphistry\")    \n",
    "  \n",
    "  \n",
    "Zoom in to search for interesting subgraphs.\n",
    "\n",
    "![Network Mapping Graphistry Image 2](images/graphistry2.png \"Network Mapping using Graphistry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Graphistry key\n",
    "# A graphistry instance is required to proceed. Please enter your own graphistry key and server information in the line below.\n",
    "# Please visit https://www.graphistry.com/ for more information on Graphistry.\n",
    "graphistry.register(key='', \n",
    "                    protocol='http', server='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_edges_pd = edges_gdf.to_pandas()\n",
    "g_edges_pd = g_edges_pd.drop(columns=['dst_idx', 'dst_node_type', 'src_idx', 'src_node_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nodes_pd = all_nodes_gdf.to_pandas()\n",
    "acct_nodes_pd = g_nodes_pd[g_nodes_pd['node_type']=='account'].assign(color=228004, icon=\"user\")\n",
    "addr_nodes_pd = g_nodes_pd[g_nodes_pd['node_type']=='address'].assign(color=228010, icon=\"desktop\")\n",
    "g_nodes_pd = pd.concat([acct_nodes_pd, addr_nodes_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graphistry.edges(g_edges_pd) \\\n",
    "                .bind(source='src', destination='dst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes(g_nodes_pd).bind(node='id', point_color='color').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
