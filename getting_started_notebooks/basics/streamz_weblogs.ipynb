{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Log Processing on GPUs\n",
    "\n",
    "Almost since the coining of the phrase \"big data\", log-processing has been a primary use-case for analytics platforms.\n",
    "\n",
    "Logs are *voluminous*:\n",
    "\n",
    "A single website visit can result in 10s to 100s of log entries, each with lengthy strings of duplicated client information.\n",
    "\n",
    "They're *complex*:\n",
    "Extracting user activities often requires combining multiple records by time and unique session identifier(s).\n",
    "\n",
    "They're *time-sensitive*:\n",
    "When something goes wrong, you need to know quickly.\n",
    "\n",
    "While early big data architectures were oriented towards batch jobs, the focus has shifted to lower-latency solutions. Distributed data processing tools and APIs have made it easier for developers to write _streaming_ applications.\n",
    "\n",
    "Below we provide an example of how to do streaming web-log processing with RAPIDS, Dask, and Streamz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Requisites\n",
    "\n",
    "We assume you're running in a RAPIDS nightly or release container, and thus already have cuDF and Dask installed.\n",
    "\n",
    "Make sure you have [streamz](https://github.com/python-streamz/streamz) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y streamz ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For demonstration purposes, we'll use a [publicly available web-log dataset from NASA](http://opensource.indeedeng.io/imhotep/docs/sample-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request, gzip, io\n",
    "\n",
    "data_dir = '/rapidsai/notebooks-extended/getting_started_notebooks/basics/'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "url = 'http://indeedeng.github.io/imhotep/files/nasa_19950630.22-19950728.12.tsv.gz'\n",
    "fn = 'logs_noheader.tsv'\n",
    "\n",
    "fileStream = io.BytesIO(urllib.request.urlopen(url).read())\n",
    "\n",
    "# We remove the header line to avoid sending it in some batches and not others\n",
    "with gzip.open(fileStream, 'rb') as f_in, open(data_dir + fn, 'wb') as fout:\n",
    "    # This is a latin character set so we must re-encode it\n",
    "    data = f_in.read().decode('iso-8859-1')\n",
    "    p_data = data.partition('\\n')\n",
    "    names = p_data[0].split()\n",
    "    fout.write(p_data[2].encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data\n",
    "\n",
    "The Google SRE HandBook says it's a good idea to track the [4 Golden Signals](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals) for any important system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "df = cudf.read_csv(data_dir + fn, sep='\\t', names=names)\n",
    "df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above doesn't tell us anything about request latency, but we can aggregate it to get a view into traffic, errors, and saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total requests served per host system\n",
    "traffic = df.groupby(['host']).host.count()\n",
    "traffic[traffic > 5].head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count HTTP error codes per host system\n",
    "errors = df[df['response'] >= 500].groupby(['host', 'response']).host.count()\n",
    "errors.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure possible saturation of host network cards\n",
    "mb_sent = df.groupby(['host']).bytes.sum()/1000000\n",
    "mb_sent[mb_sent > 100].head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the above that there are not many errors which is great and we can also see hits per host and total MBs sent per host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU Streaming with RAPIDS and Streamz\n",
    "\n",
    "A single GPU can process a lot of data quickly. Thanks to the Streamz API, it's also easy to do it in streaming fashion.\n",
    "\n",
    "In many streaming systems you return events of interest for ops teams to investigate. That is what we will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO, StringIO\n",
    "\n",
    "# calculate traffic, errors, and saturation per batch\n",
    "def process_on_gpu(messages):\n",
    "    # Check if input message stream is decoded string or utf-8 encoded bytes object\n",
    "    try:\n",
    "        message_stream = StringIO(('\\n').join(messages))\n",
    "    except:\n",
    "        message_stream = BytesIO(b'\\n'.join(messages))\n",
    "                       \n",
    "    df = cudf.read_csv(message_stream, sep='\\t', names=names)\n",
    "    traffic = df.groupby(['host']).host.count()\n",
    "    errors = df[df['response'] >= 500].groupby(['host', 'response']).host.count()\n",
    "    mb_sent = df.groupby(['host']).bytes.sum()/1000000\n",
    "\n",
    "    # Return - TSV versions of each metric\n",
    "    return {'traffic': str(traffic[traffic > 200]), 'errors': str(errors), 'mb_sent': str(mb_sent[mb_sent > 120])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "# save each metric type to its own file, instead of dumping lots of output to Jupyter\n",
    "def save_to_file(events):\n",
    "    dt = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(data_dir + 'traffic.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['traffic'])\n",
    "    with open(data_dir + 'errors.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['errors'])\n",
    "    with open(data_dir + 'mb_sent.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['mb_sent'])\n",
    "    print(str(dt) + ': metrics batch written..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamz import Stream\n",
    "\n",
    "# setup the stream\n",
    "source = Stream.from_textfile(data_dir + fn)\n",
    "# process 250k lines per batch\n",
    "out = source.partition(250000).map(process_on_gpu).sink(save_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream data from file. This is a slow means of emitting data to a stream\n",
    "# see below for a faster approach with Kafka\n",
    "with open(data_dir + fn, 'rb') as fp:\n",
    "    for line in fp.readlines():\n",
    "        source.emit(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Error Log:\"\n",
    "!head {data_dir}errors.txt\n",
    "!echo \"\\nTraffic Log:\"\n",
    "!head {data_dir}traffic.txt\n",
    "!echo \"\\nMB Sent Log:\"\n",
    "!head {data_dir}mb_sent.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Streamz to multiple GPUs with Dask & Kafka\n",
    "\n",
    "As opposed to streaming from files a very common pattern is to read from distributed log systems like Apache Kafka.\n",
    "\n",
    "The below example assumes you have a running Kafka instance/cluster.\n",
    "\n",
    "For help setting up your own, follow the [Kafka Quickstart guide](http://kafka.apache.org/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# create a Dask cluster with 1 worker per GPU\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamz import Stream\n",
    "import confluent_kafka\n",
    "\n",
    "# Kafka specific configurations\n",
    "topic = \"haproxy-topic\"\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "consumer_conf = {'bootstrap.servers': bootstrap_servers, 'group.id': 'custreamz', 'session.timeout.ms': 60000}\n",
    "\n",
    "stream = Stream.from_kafka_batched(topic, consumer_conf, poll_interval='1s', npartitions=4, asynchronous=True, dask=False)\n",
    "final_output = stream.map(process_on_gpu).sink(print)\n",
    "stream.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
